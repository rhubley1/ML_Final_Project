{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Final Project -- Personal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlB5ULxXnxAv",
        "outputId": "2294172d-d159-475a-c84e-86fb9c4733db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "import pandas as pd\n",
        "#files located in root\n",
        "amazon_df = pd.read_csv('/content/gdrive/My Drive/amazon_cells_labelled.txt', sep='\\t', header=None)\n",
        "yelp_df = pd.read_csv('/content/gdrive/My Drive/yelp_labelled.txt', sep='\\t', header=None)\n",
        "imdb_df = pd.read_csv('/content/gdrive/My Drive/imdb_labelled.txt', sep='\\t', header=None)\n",
        "df = pd.concat([amazon_df, yelp_df, imdb_df]) #all reviews from the 3 different sources\n",
        "df.columns = ['Sentence', 'Label']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnX8D9qoAW-B"
      },
      "source": [
        "Preprocess Data! Convert to Lower Case, Remove Punctuation, Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96DjiKhqCxOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958cfdbc-2d04-40d6-b1d7-8fe290cb0a41"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "meaninglessWords=list(stopwords.words('english'))\n",
        "#Imported necessary pre-processing tools\n",
        "\n",
        "def preprocess_func(sentences):\n",
        "  \"\"\" \n",
        "  args: sentences => List of sentences that need to be preprocessed\n",
        "  returns: cleaned_sentences => List of cleaned sentences that are lower-case, punctuation removed, and stopwords removed\"\"\"\n",
        "  \n",
        "  cleaned_sentences = []\n",
        "  sents = sentences\n",
        "  for i in range(len(sents)):\n",
        "    sent = sents[i]\n",
        "    sent = sent.lower() #convert to lowercase\n",
        "    for char in sent:\n",
        "      if char in string.punctuation:\n",
        "        sent=sent.replace(char,\"\") #remove punctutation\n",
        "    words = sent.split()\n",
        "    sent = []\n",
        "    for word in words:\n",
        "      if word not in meaninglessWords:\n",
        "        sent.append(word)\n",
        "    sent = \" \".join(sent) #remove stopwords\n",
        "    cleaned_sentences.append(sent)\n",
        "  return cleaned_sentences\n",
        "\n",
        "cleaned_sentencesAmazon = preprocess_func(list(amazon_df[0]))\n",
        "cleaned_sentencesYelp = preprocess_func(list(yelp_df[0]))\n",
        "cleaned_sentencesIMDB = preprocess_func(list(imdb_df[0]))\n",
        "amazon_df['Cleaned Sentences']=cleaned_sentencesAmazon\n",
        "yelp_df['Cleaned Sentences'] = cleaned_sentencesYelp\n",
        "imdb_df['Cleaned Sentences'] = cleaned_sentencesIMDB\n",
        "amazon_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
        "amazon_df = amazon_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
        "yelp_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
        "yelp_df = yelp_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
        "imdb_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
        "imdb_df = imdb_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLr7OPHJqBqi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "5bd8378c-21b4-424d-fa7a-9b4cb8aefebb"
      },
      "source": [
        "df = pd.concat([amazon_df, yelp_df, imdb_df]) #all reviews from the 3 different sources\n",
        "df.columns = ['Sentence', 'Cleaned Sentence', 'Label']\n",
        "df = df.drop(labels='Sentence',axis=1)\n",
        "df = df.sample(frac=1) #randomly shuffle rows\n",
        "import numpy as np\n",
        "#reset indices of dataframe \n",
        "idx = np.arange(0,df.shape[0])\n",
        "df=df.set_index(idx)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cleaned Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>much better ayce sushi place went vegas</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>owners really great people</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>able voice dialing car problem</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wasted little money earpiece</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>first charge kept going dead 12 minutes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>predictable even chick flick</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2744</th>\n",
              "      <td>star trek v final frontier worst series</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2745</th>\n",
              "      <td>even squibs look awful</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2746</th>\n",
              "      <td>chow mein good</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2747</th>\n",
              "      <td>almost unbearable watch screen little charisma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2748 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Cleaned Sentence  Label\n",
              "0               much better ayce sushi place went vegas      1\n",
              "1                            owners really great people      1\n",
              "2                        able voice dialing car problem      1\n",
              "3                          wasted little money earpiece      0\n",
              "4               first charge kept going dead 12 minutes      0\n",
              "...                                                 ...    ...\n",
              "2743                       predictable even chick flick      0\n",
              "2744            star trek v final frontier worst series      0\n",
              "2745                             even squibs look awful      0\n",
              "2746                                     chow mein good      1\n",
              "2747  almost unbearable watch screen little charisma...      0\n",
              "\n",
              "[2748 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHHFYd9VIb91"
      },
      "source": [
        "df.columns=['Sentence','Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGewRLDHlKD"
      },
      "source": [
        "So, we have preprocessed all 3 review datasets that comprise our original dataset. Then, we put them all into one big dataframe called df and shuffled them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FohuOXE_93B7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FqGbbjysQMB"
      },
      "source": [
        "#converts sentences into tdfid vectors using varying n-gram sizes\n",
        "cvModUnigram = CountVectorizer()\n",
        "cvModBigram = CountVectorizer(ngram_range=(2,2))\n",
        "cvModTrigram = CountVectorizer(ngram_range=(3,3))\n",
        "cvModFourgram = CountVectorizer(ngram_range=(4,4))\n",
        "cvModUniAndBi = CountVectorizer(ngram_range=(1,2))\n",
        "cvModUniBiAndTri = CountVectorizer(ngram_range=(1,3))\n",
        "countvectorsUnigram = cvModUnigram.fit_transform(df['Sentence'])\n",
        "countvectorsBigram = cvModBigram.fit_transform(df['Sentence'])\n",
        "countvectorsTrigram = cvModTrigram.fit_transform(df['Sentence'])\n",
        "countvectorsFourgram = cvModFourgram.fit_transform(df['Sentence'])\n",
        "countvectorsUniAndBi = cvModUniAndBi.fit_transform(df['Sentence'])\n",
        "countvectorsUniBiAndTri = cvModUniBiAndTri.fit_transform(df['Sentence'])\n",
        "featureAsWordsUnigram = cvModUnigram.get_feature_names()\n",
        "featuresBigram = cvModBigram.get_feature_names()\n",
        "featuresTrigram = cvModTrigram.get_feature_names()\n",
        "featuresFourgram = cvModFourgram.get_feature_names()\n",
        "featureUniAndBi = cvModUniAndBi.get_feature_names()\n",
        "featuresUniBiAndTri = cvModUniBiAndTri.get_feature_names()\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidfMod =TfidfTransformer()\n",
        "tfidfVecsUnigram = tfidfMod.fit_transform(countvectorsUnigram,y=None)\n",
        "tfidfVecsBigram = tfidfMod.fit_transform(countvectorsBigram, y=None)\n",
        "tfidfVecsTrigram = tfidfMod.fit_transform(countvectorsTrigram, y=None) \n",
        "tfidfVecsFourgram = tfidfMod.fit_transform(countvectorsFourgram, y=None)\n",
        "tfidfVecsUniAndBi = tfidfMod.fit_transform(countvectorsUniAndBi, y=None)\n",
        "tfidfVecsUniBiAndTri = tfidfMod.fit_transform(countvectorsUniBiAndTri, y=None)\n",
        "tfidfVecsUnigram = tfidfVecsUnigram.toarray() #unigram tfidf vectors\n",
        "tfidfVecsBigram = tfidfVecsBigram.toarray() #bigram tfidf vectors\n",
        "tfidfVecsTrigram = tfidfVecsTrigram.toarray() #trigram\n",
        "tfidfVecsFourgram = tfidfVecsFourgram.toarray() #fourgram\n",
        "tfidfVecsUniAndBi = tfidfVecsUniAndBi.toarray() #unigram and bigram\n",
        "tfidfVecsUniBiAndTri = tfidfVecsUniBiAndTri.toarray() #uni, bi, and trigram\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s17ClKlVviDf"
      },
      "source": [
        "columnsUni = [num for num in range(tfidfVecsUnigram.shape[1])]\n",
        "columnsBi = [num for num in range(tfidfVecsBigram.shape[1])]\n",
        "columnsTri = [num for num in range(tfidfVecsTrigram.shape[1])]\n",
        "columnsFour = [num for num in range(tfidfVecsFourgram.shape[1])]\n",
        "columnsUniAndBi = [num for num in range(tfidfVecsUniAndBi.shape[1])]\n",
        "columnsUniBiAndTri = [num for num in range(tfidfVecsUniBiAndTri.shape[1])]\n",
        "index = [num for num in range(tfidfVecsUnigram.shape[0])]\n",
        "tfidfUniDF = pd.DataFrame(data=tfidfVecsUnigram,index=index,columns=columnsUni)\n",
        "tfidfBiDF = pd.DataFrame(data=tfidfVecsBigram,index=index, columns=columnsBi)\n",
        "tfidfTriDF = pd.DataFrame(data=tfidfVecsTrigram,index=index, columns=columnsTri)\n",
        "tfidfFourDF = pd.DataFrame(data=tfidfVecsFourgram, index = index, columns=columnsFour)\n",
        "tfidfUniAndBiDF = pd.DataFrame(data=tfidfVecsUniAndBi,index=index, columns=columnsUniAndBi)\n",
        "tfidfUniBiAndTriDF = pd.DataFrame(data=tfidfVecsUniBiAndTri,index=index,columns=columnsUniBiAndTri)\n",
        "tfidfUniDF.columns=featureAsWordsUnigram\n",
        "tfidfBiDF.columns =featuresBigram\n",
        "tfidfTriDF.columns = featuresTrigram\n",
        "tfidfFourDF.columns = featuresFourgram\n",
        "tfidfUniAndBiDF.columns = featureUniAndBi\n",
        "tfidfUniBiAndTriDF.columns = featuresUniBiAndTri\n",
        "tfidfUniDF.insert(tfidfUniDF.shape[1],'Label',df['Label'])\n",
        "tfidfBiDF.insert(tfidfBiDF.shape[1],'Label',df['Label'])\n",
        "tfidfTriDF.insert(tfidfTriDF.shape[1],'Label',df['Label'])\n",
        "tfidfFourDF.insert(tfidfFourDF.shape[1],'Label',df['Label'])\n",
        "tfidfUniAndBiDF.insert(tfidfUniAndBiDF.shape[1],'Label',df['Label'])\n",
        "tfidfUniBiAndTriDF.insert(tfidfUniBiAndTriDF.shape[1],'Label',df['Label'])\n",
        "\n",
        "# creates dataframes of all the different n-gram tfidf vectors with the appropriate feature names and the label column as the rightmost column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "8ovrCiTRe7Uc",
        "outputId": "6c4bf10b-0a51-42bb-dd8e-3a95634b1ffa"
      },
      "source": [
        "tfidfUniAndBiDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>010</th>\n",
              "      <th>010 grade</th>\n",
              "      <th>10</th>\n",
              "      <th>10 10</th>\n",
              "      <th>10 110</th>\n",
              "      <th>10 amazing</th>\n",
              "      <th>10 feet</th>\n",
              "      <th>10 minutes</th>\n",
              "      <th>10 movie</th>\n",
              "      <th>10 oyvey</th>\n",
              "      <th>10 plus</th>\n",
              "      <th>10 saved</th>\n",
              "      <th>10 simply</th>\n",
              "      <th>10 stars</th>\n",
              "      <th>10 think</th>\n",
              "      <th>10 times</th>\n",
              "      <th>10 years</th>\n",
              "      <th>100</th>\n",
              "      <th>100 functional</th>\n",
              "      <th>100 recommended</th>\n",
              "      <th>100 times</th>\n",
              "      <th>1010</th>\n",
              "      <th>11</th>\n",
              "      <th>11 months</th>\n",
              "      <th>110</th>\n",
              "      <th>110 scale</th>\n",
              "      <th>110 setting</th>\n",
              "      <th>1199</th>\n",
              "      <th>1199 sandwich</th>\n",
              "      <th>12</th>\n",
              "      <th>12 hours</th>\n",
              "      <th>12 mega</th>\n",
              "      <th>12 mile</th>\n",
              "      <th>12 minutes</th>\n",
              "      <th>12 ridiculous</th>\n",
              "      <th>12 years</th>\n",
              "      <th>13</th>\n",
              "      <th>13 bucks</th>\n",
              "      <th>13 megapixels</th>\n",
              "      <th>15</th>\n",
              "      <th>...</th>\n",
              "      <th>younger</th>\n",
              "      <th>younger set</th>\n",
              "      <th>youre</th>\n",
              "      <th>youre familiar</th>\n",
              "      <th>youre gonna</th>\n",
              "      <th>youre looking</th>\n",
              "      <th>youre outrageously</th>\n",
              "      <th>youre served</th>\n",
              "      <th>youre visually</th>\n",
              "      <th>youthful</th>\n",
              "      <th>youthful energy</th>\n",
              "      <th>youtube</th>\n",
              "      <th>youve</th>\n",
              "      <th>youve must</th>\n",
              "      <th>yucky</th>\n",
              "      <th>yukon</th>\n",
              "      <th>yukon gold</th>\n",
              "      <th>yum</th>\n",
              "      <th>yum sauce</th>\n",
              "      <th>yum yum</th>\n",
              "      <th>yummy</th>\n",
              "      <th>yummy christmas</th>\n",
              "      <th>yummy try</th>\n",
              "      <th>yummy tummy</th>\n",
              "      <th>yun</th>\n",
              "      <th>yun fat</th>\n",
              "      <th>z500a</th>\n",
              "      <th>z500a im</th>\n",
              "      <th>zero</th>\n",
              "      <th>zero stars</th>\n",
              "      <th>zero taste</th>\n",
              "      <th>zillion</th>\n",
              "      <th>zillion times</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie movies</th>\n",
              "      <th>zombiestudents</th>\n",
              "      <th>zombiestudents back</th>\n",
              "      <th>zombiez</th>\n",
              "      <th>zombiez part</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2646</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.318231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2744</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2745</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2746</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2747</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2748 rows Ã— 19639 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      010  010 grade   10  ...  zombiez  zombiez part  Label\n",
              "0     0.0        0.0  0.0  ...      0.0           0.0      1\n",
              "1     0.0        0.0  0.0  ...      0.0           0.0      1\n",
              "2     0.0        0.0  0.0  ...      0.0           0.0      1\n",
              "3     0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "4     0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "...   ...        ...  ...  ...      ...           ...    ...\n",
              "2743  0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "2744  0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "2745  0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "2746  0.0        0.0  0.0  ...      0.0           0.0      1\n",
              "2747  0.0        0.0  0.0  ...      0.0           0.0      0\n",
              "\n",
              "[2748 rows x 19639 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3nsimsVPAT4"
      },
      "source": [
        "Note, I created varying sizes of n-gram tfidf vectors for the shuffled dataframe but not for the individual specific dataframes (individual specific = amazon, yelp, and imdb). Will do that now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8ZHm3alzPbSq",
        "outputId": "eb913f86-35ba-4890-caf7-bb352754ad89"
      },
      "source": [
        "amazon_df=amazon_df.drop(labels='Sentence',axis=1)\n",
        "amazon_df.columns=['Sentence','Label']\n",
        "amazon_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>way plug us unless go converter</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>good case excellent value</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>great jawbone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tied charger conversations lasting 45 minutesm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mic great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>screen get smudged easily touches ear face</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>piece junk lose calls phone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>item match picture</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>thing disappoint infra red port irda</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>answer calls unit never worked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Label\n",
              "0                      way plug us unless go converter      0\n",
              "1                            good case excellent value      1\n",
              "2                                        great jawbone      1\n",
              "3    tied charger conversations lasting 45 minutesm...      0\n",
              "4                                            mic great      1\n",
              "..                                                 ...    ...\n",
              "995         screen get smudged easily touches ear face      0\n",
              "996                        piece junk lose calls phone      0\n",
              "997                                 item match picture      0\n",
              "998               thing disappoint infra red port irda      0\n",
              "999                     answer calls unit never worked      0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "a0OlXwHdPv4H",
        "outputId": "79c99c66-0033-4ab9-fb50-40ff895c053a"
      },
      "source": [
        "yelp_df = yelp_df.drop(labels='Sentence', axis=1)\n",
        "yelp_df.columns=['Sentence','Label']\n",
        "yelp_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow loved place</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crust good</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tasty texture nasty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stopped late may bank holiday rick steve recom...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>selection menu great prices</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>think food flavor texture lacking</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>appetite instantly gone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>overall impressed would go back</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>whole experience underwhelming think well go n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>hadnt wasted enough life poured salt wound dra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Label\n",
              "0                                      wow loved place      1\n",
              "1                                           crust good      0\n",
              "2                                  tasty texture nasty      0\n",
              "3    stopped late may bank holiday rick steve recom...      1\n",
              "4                          selection menu great prices      1\n",
              "..                                                 ...    ...\n",
              "995                  think food flavor texture lacking      0\n",
              "996                            appetite instantly gone      0\n",
              "997                    overall impressed would go back      0\n",
              "998  whole experience underwhelming think well go n...      0\n",
              "999  hadnt wasted enough life poured salt wound dra...      0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "e3678oLOP8HX",
        "outputId": "f1cdff63-490e-45a3-90b3-464c0df83b86"
      },
      "source": [
        "imdb_df=imdb_df.drop(labels='Sentence',axis=1)\n",
        "imdb_df.columns=['Sentence','Label']\n",
        "imdb_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sure lost flat characters audience nearly half...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attempting artiness black white clever camera ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>little music anything speak</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best scene movie gerardo trying find song keep...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>got bored watching jessice lange take clothes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>unfortunately virtue films production work los...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>word embarrassing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>exceptionally bad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>insult ones intelligence huge waste money</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>748 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Sentence  Label\n",
              "0    slowmoving aimless movie distressed drifting y...      0\n",
              "1    sure lost flat characters audience nearly half...      0\n",
              "2    attempting artiness black white clever camera ...      0\n",
              "3                          little music anything speak      0\n",
              "4    best scene movie gerardo trying find song keep...      1\n",
              "..                                                 ...    ...\n",
              "743      got bored watching jessice lange take clothes      0\n",
              "744  unfortunately virtue films production work los...      0\n",
              "745                                  word embarrassing      0\n",
              "746                                  exceptionally bad      0\n",
              "747          insult ones intelligence huge waste money      0\n",
              "\n",
              "[748 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWawYbKRQVMn"
      },
      "source": [
        "AmazoncountvectorsUnigram = cvModUnigram.fit_transform(amazon_df['Sentence'])\n",
        "AmazoncountvectorsBigram = cvModBigram.fit_transform(amazon_df['Sentence'])\n",
        "AmazoncountvectorsTrigram = cvModTrigram.fit_transform(amazon_df['Sentence'])\n",
        "AmazoncountvectorsFourgram = cvModFourgram.fit_transform(amazon_df['Sentence'])\n",
        "AmazoncountvectorsUniAndBi = cvModUniAndBi.fit_transform(amazon_df['Sentence'])\n",
        "AmazoncountvectorsUniBiAndTri = cvModUniBiAndTri.fit_transform(amazon_df['Sentence'])\n",
        "\n",
        "AmazontfidfVecsUnigram = tfidfMod.fit_transform(AmazoncountvectorsUnigram,y=None)\n",
        "AmazontfidfVecsBigram = tfidfMod.fit_transform(AmazoncountvectorsBigram, y=None)\n",
        "AmazontfidfVecsTrigram = tfidfMod.fit_transform(AmazoncountvectorsTrigram, y=None) \n",
        "AmazontfidfVecsFourgram = tfidfMod.fit_transform(AmazoncountvectorsFourgram, y=None)\n",
        "AmazontfidfVecsUniAndBi = tfidfMod.fit_transform(AmazoncountvectorsUniAndBi, y=None)\n",
        "AmazontfidfVecsUniBiAndTri = tfidfMod.fit_transform(AmazoncountvectorsUniBiAndTri, y=None)\n",
        "AmazontfidfVecsUnigram = AmazontfidfVecsUnigram.toarray() #unigram tfidf vectors\n",
        "AmazontfidfVecsBigram = AmazontfidfVecsBigram.toarray() #bigram tfidf vectors\n",
        "AmazontfidfVecsTrigram = AmazontfidfVecsTrigram.toarray() #trigram\n",
        "AmazontfidfVecsFourgram = AmazontfidfVecsFourgram.toarray() #fourgram\n",
        "AmazontfidfVecsUniAndBi = AmazontfidfVecsUniAndBi.toarray() #unigram and bigram\n",
        "AmazontfidfVecsUniBiAndTri = AmazontfidfVecsUniBiAndTri.toarray() #uni, bi, and trigram\n",
        "\n",
        "AmazoncolumnsUni = [num for num in range(AmazontfidfVecsUnigram.shape[1])]\n",
        "AmazoncolumnsBi = [num for num in range(AmazontfidfVecsBigram.shape[1])]\n",
        "AmazoncolumnsTri = [num for num in range(AmazontfidfVecsTrigram.shape[1])]\n",
        "AmazoncolumnsFour = [num for num in range(AmazontfidfVecsFourgram.shape[1])]\n",
        "AmazoncolumnsUniAndBi = [num for num in range(AmazontfidfVecsUniAndBi.shape[1])]\n",
        "AmazoncolumnsUniBiAndTri = [num for num in range(AmazontfidfVecsUniBiAndTri.shape[1])]\n",
        "\n",
        "Amazonindex = [num for num in range(AmazontfidfVecsUnigram.shape[0])]\n",
        "\n",
        "\n",
        "AmazontfidfUniDF = pd.DataFrame(data=AmazontfidfVecsUnigram,index=Amazonindex,columns=AmazoncolumnsUni)\n",
        "AmazontfidfBiDF = pd.DataFrame(data=AmazontfidfVecsBigram,index=Amazonindex, columns=AmazoncolumnsBi)\n",
        "AmazontfidfTriDF = pd.DataFrame(data=AmazontfidfVecsTrigram,index=Amazonindex, columns=AmazoncolumnsTri)\n",
        "AmazontfidfFourDF = pd.DataFrame(data=AmazontfidfVecsFourgram, index = Amazonindex, columns=AmazoncolumnsFour)\n",
        "AmazontfidfUniAndBiDF = pd.DataFrame(data=AmazontfidfVecsUniAndBi,index=Amazonindex, columns=AmazoncolumnsUniAndBi)\n",
        "AmazontfidfUniBiAndTriDF = pd.DataFrame(data=AmazontfidfVecsUniBiAndTri,index=Amazonindex,columns=AmazoncolumnsUniBiAndTri)\n",
        "\n",
        "AmazontfidfUniDF.insert(AmazontfidfUniDF.shape[1],'Label',amazon_df['Label'])\n",
        "AmazontfidfBiDF.insert(AmazontfidfBiDF.shape[1],'Label',amazon_df['Label'])\n",
        "AmazontfidfTriDF.insert(AmazontfidfTriDF.shape[1],'Label',amazon_df['Label'])\n",
        "AmazontfidfFourDF.insert(AmazontfidfFourDF.shape[1],'Label',amazon_df['Label'])\n",
        "AmazontfidfUniAndBiDF.insert(AmazontfidfUniAndBiDF.shape[1],'Label',amazon_df['Label'])\n",
        "AmazontfidfUniBiAndTriDF.insert(AmazontfidfUniBiAndTriDF.shape[1],'Label',amazon_df['Label'])\n",
        "\n",
        "YelpcountvectorsUnigram = cvModUnigram.fit_transform(yelp_df['Sentence'])\n",
        "YelpcountvectorsBigram = cvModBigram.fit_transform(yelp_df['Sentence'])\n",
        "YelpcountvectorsTrigram = cvModTrigram.fit_transform(yelp_df['Sentence'])\n",
        "YelpcountvectorsFourgram = cvModFourgram.fit_transform(yelp_df['Sentence'])\n",
        "YelpcountvectorsUniAndBi = cvModUniAndBi.fit_transform(yelp_df['Sentence'])\n",
        "YelpcountvectorsUniBiAndTri = cvModUniBiAndTri.fit_transform(yelp_df['Sentence'])\n",
        "\n",
        "YelptfidfVecsUnigram = tfidfMod.fit_transform(YelpcountvectorsUnigram,y=None)\n",
        "YelptfidfVecsBigram = tfidfMod.fit_transform(YelpcountvectorsBigram, y=None)\n",
        "YelptfidfVecsTrigram = tfidfMod.fit_transform(YelpcountvectorsTrigram, y=None) \n",
        "YelptfidfVecsFourgram = tfidfMod.fit_transform(YelpcountvectorsFourgram, y=None)\n",
        "YelptfidfVecsUniAndBi = tfidfMod.fit_transform(YelpcountvectorsUniAndBi, y=None)\n",
        "YelptfidfVecsUniBiAndTri = tfidfMod.fit_transform(YelpcountvectorsUniBiAndTri, y=None)\n",
        "YelptfidfVecsUnigram = YelptfidfVecsUnigram.toarray() #unigram tfidf vectors\n",
        "YelptfidfVecsBigram = YelptfidfVecsBigram.toarray() #bigram tfidf vectors\n",
        "YelptfidfVecsTrigram = YelptfidfVecsTrigram.toarray() #trigram\n",
        "YelptfidfVecsFourgram = YelptfidfVecsFourgram.toarray() #fourgram\n",
        "YelptfidfVecsUniAndBi = YelptfidfVecsUniAndBi.toarray() #unigram and bigram\n",
        "YelptfidfVecsUniBiAndTri = YelptfidfVecsUniBiAndTri.toarray() #uni, bi, and trigram\n",
        "\n",
        "YelpcolumnsUni = [num for num in range(YelptfidfVecsUnigram.shape[1])]\n",
        "YelpcolumnsBi = [num for num in range(YelptfidfVecsBigram.shape[1])]\n",
        "YelpcolumnsTri = [num for num in range(YelptfidfVecsTrigram.shape[1])]\n",
        "YelpcolumnsFour = [num for num in range(YelptfidfVecsFourgram.shape[1])]\n",
        "YelpcolumnsUniAndBi = [num for num in range(YelptfidfVecsUniAndBi.shape[1])]\n",
        "YelpcolumnsUniBiAndTri = [num for num in range(YelptfidfVecsUniBiAndTri.shape[1])]\n",
        "\n",
        "Yelpindex = [num for num in range(YelptfidfVecsUnigram.shape[0])]\n",
        "\n",
        "YelptfidfUniDF = pd.DataFrame(data=YelptfidfVecsUnigram,index=Yelpindex,columns=YelpcolumnsUni)\n",
        "YelptfidfBiDF = pd.DataFrame(data=YelptfidfVecsBigram,index=Yelpindex, columns=YelpcolumnsBi)\n",
        "YelptfidfTriDF = pd.DataFrame(data=YelptfidfVecsTrigram,index=Yelpindex, columns=YelpcolumnsTri)\n",
        "YelptfidfFourDF = pd.DataFrame(data=YelptfidfVecsFourgram, index =Yelpindex, columns=YelpcolumnsFour)\n",
        "YelptfidfUniAndBiDF = pd.DataFrame(data=YelptfidfVecsUniAndBi,index=Yelpindex, columns=YelpcolumnsUniAndBi)\n",
        "YelptfidfUniBiAndTriDF = pd.DataFrame(data=YelptfidfVecsUniBiAndTri,index=Yelpindex,columns=YelpcolumnsUniBiAndTri)\n",
        "\n",
        "YelptfidfUniDF.insert(YelptfidfUniDF.shape[1],'Label',yelp_df['Label'])\n",
        "YelptfidfBiDF.insert(YelptfidfBiDF.shape[1],'Label',yelp_df['Label'])\n",
        "YelptfidfTriDF.insert(YelptfidfTriDF.shape[1],'Label',yelp_df['Label'])\n",
        "YelptfidfFourDF.insert(YelptfidfFourDF.shape[1],'Label',yelp_df['Label'])\n",
        "YelptfidfUniAndBiDF.insert(YelptfidfUniAndBiDF.shape[1],'Label',yelp_df['Label'])\n",
        "YelptfidfUniBiAndTriDF.insert(YelptfidfUniBiAndTriDF.shape[1],'Label',yelp_df['Label'])\n",
        "\n",
        "IMDBcountvectorsUnigram = cvModUnigram.fit_transform(imdb_df['Sentence'])\n",
        "IMDBcountvectorsBigram = cvModBigram.fit_transform(imdb_df['Sentence'])\n",
        "IMDBcountvectorsTrigram = cvModTrigram.fit_transform(imdb_df['Sentence'])\n",
        "IMDBcountvectorsFourgram = cvModFourgram.fit_transform(imdb_df['Sentence'])\n",
        "IMDBcountvectorsUniAndBi = cvModUniAndBi.fit_transform(imdb_df['Sentence'])\n",
        "IMDBcountvectorsUniBiAndTri = cvModUniBiAndTri.fit_transform(imdb_df['Sentence'])\n",
        "\n",
        "IMDBtfidfVecsUnigram = tfidfMod.fit_transform(IMDBcountvectorsUnigram,y=None)\n",
        "IMDBtfidfVecsBigram = tfidfMod.fit_transform(IMDBcountvectorsBigram, y=None)\n",
        "IMDBtfidfVecsTrigram = tfidfMod.fit_transform(IMDBcountvectorsTrigram, y=None) \n",
        "IMDBtfidfVecsFourgram = tfidfMod.fit_transform(IMDBcountvectorsFourgram, y=None)\n",
        "IMDBtfidfVecsUniAndBi = tfidfMod.fit_transform(IMDBcountvectorsUniAndBi, y=None)\n",
        "IMDBtfidfVecsUniBiAndTri = tfidfMod.fit_transform(IMDBcountvectorsUniBiAndTri, y=None)\n",
        "IMDBtfidfVecsUnigram = IMDBtfidfVecsUnigram.toarray() #unigram tfidf vectors\n",
        "IMDBtfidfVecsBigram = IMDBtfidfVecsBigram.toarray() #bigram tfidf vectors\n",
        "IMDBtfidfVecsTrigram = IMDBtfidfVecsTrigram.toarray() #trigram\n",
        "IMDBtfidfVecsFourgram = IMDBtfidfVecsFourgram.toarray() #fourgram\n",
        "IMDBtfidfVecsUniAndBi = IMDBtfidfVecsUniAndBi.toarray() #unigram and bigram\n",
        "IMDBtfidfVecsUniBiAndTri = IMDBtfidfVecsUniBiAndTri.toarray() #uni, bi, and trigram\n",
        "\n",
        "IMDBcolumnsUni = [num for num in range(IMDBtfidfVecsUnigram.shape[1])]\n",
        "IMDBcolumnsBi = [num for num in range(IMDBtfidfVecsBigram.shape[1])]\n",
        "IMDBcolumnsTri = [num for num in range(IMDBtfidfVecsTrigram.shape[1])]\n",
        "IMDBcolumnsFour = [num for num in range(IMDBtfidfVecsFourgram.shape[1])]\n",
        "IMDBcolumnsUniAndBi = [num for num in range(IMDBtfidfVecsUniAndBi.shape[1])]\n",
        "IMDBcolumnsUniBiAndTri = [num for num in range(IMDBtfidfVecsUniBiAndTri.shape[1])]\n",
        "\n",
        "IMDBindex = [num for num in range(IMDBtfidfVecsUnigram.shape[0])]\n",
        "\n",
        "IMDBtfidfUniDF = pd.DataFrame(data=IMDBtfidfVecsUnigram,index=IMDBindex,columns=IMDBcolumnsUni)\n",
        "IMDBtfidfBiDF = pd.DataFrame(data=IMDBtfidfVecsBigram,index=IMDBindex, columns=IMDBcolumnsBi)\n",
        "IMDBtfidfTriDF = pd.DataFrame(data=IMDBtfidfVecsTrigram,index=IMDBindex, columns=IMDBcolumnsTri)\n",
        "IMDBtfidfFourDF = pd.DataFrame(data=IMDBtfidfVecsFourgram, index =IMDBindex, columns=IMDBcolumnsFour)\n",
        "IMDBtfidfUniAndBiDF = pd.DataFrame(data=IMDBtfidfVecsUniAndBi,index=IMDBindex, columns=IMDBcolumnsUniAndBi)\n",
        "IMDBtfidfUniBiAndTriDF = pd.DataFrame(data=IMDBtfidfVecsUniBiAndTri,index=IMDBindex,columns=IMDBcolumnsUniBiAndTri)\n",
        "\n",
        "IMDBtfidfUniDF.insert(IMDBtfidfUniDF.shape[1],'Label',imdb_df['Label'])\n",
        "IMDBtfidfBiDF.insert(IMDBtfidfBiDF.shape[1],'Label',imdb_df['Label'])\n",
        "IMDBtfidfTriDF.insert(IMDBtfidfTriDF.shape[1],'Label',imdb_df['Label'])\n",
        "IMDBtfidfFourDF.insert(IMDBtfidfFourDF.shape[1],'Label',imdb_df['Label'])\n",
        "IMDBtfidfUniAndBiDF.insert(IMDBtfidfUniAndBiDF.shape[1],'Label',imdb_df['Label'])\n",
        "IMDBtfidfUniBiAndTriDF.insert(IMDBtfidfUniBiAndTriDF.shape[1],'Label',imdb_df['Label'])\n",
        "\n",
        "# Converts site-specific data into varying n-gram tfidf dataframes with label as rightmost col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDae5444WROr"
      },
      "source": [
        "So, I now have multiple dataframes corresponding to the mix of reviews and each site-specific reviews and for each of these I have unigram, bigram, etc.. tfidf dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrdX1KKSiStK"
      },
      "source": [
        "Models implemented with use of tfidf vectors of varying n-gram styles and on the different datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iapsQx2blKPg"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY2Ug4n2m13g"
      },
      "source": [
        "Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOch8PhFlQ32"
      },
      "source": [
        "tfidfUniDF = tfidfUniDF.drop(labels='Label',axis=1)\n",
        "tfidfBiDF = tfidfBiDF.drop(labels='Label', axis = 1)\n",
        "tfidfTriDF = tfidfTriDF.drop(labels='Label', axis=1)\n",
        "tfidfFourDF = tfidfFourDF.drop(labels='Label',axis=1)\n",
        "tfidfUniAndBiDF = tfidfUniAndBiDF.drop(labels='Label',axis=1)\n",
        "tfidfUniBiAndTriDF = tfidfUniBiAndTriDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfUniDF = AmazontfidfUniDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfBiDF = AmazontfidfBiDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfTriDF = AmazontfidfTriDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfFourDF = AmazontfidfFourDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfUniAndBiDF = AmazontfidfUniAndBiDF.drop(labels='Label',axis=1)\n",
        "AmazontfidfUniBiAndTriDF = AmazontfidfUniBiAndTriDF.drop(labels='Label',axis=1)\n",
        "YelptfidfUniDF = YelptfidfUniDF.drop(labels='Label',axis=1)\n",
        "YelptfidfBiDF = YelptfidfBiDF.drop(labels='Label',axis=1)\n",
        "YelptfidfTriDF = YelptfidfTriDF.drop(labels='Label',axis=1)\n",
        "YelptfidfFourDF = YelptfidfFourDF.drop(labels='Label',axis=1)\n",
        "YelptfidfUniAndBiDF = YelptfidfUniAndBiDF.drop(labels='Label',axis=1)\n",
        "YelptfidfUniBiAndTriDF = YelptfidfUniBiAndTriDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfUniDF = IMDBtfidfUniDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfBiDF = IMDBtfidfBiDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfTriDF = IMDBtfidfTriDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfFourDF = IMDBtfidfFourDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfUniAndBiDF = IMDBtfidfUniAndBiDF.drop(labels='Label',axis=1)\n",
        "IMDBtfidfUniBiAndTriDF = IMDBtfidfUniBiAndTriDF.drop(labels='Label',axis=1)\n",
        "\n",
        "#dropped the labels for all the dataframes\n",
        "\n",
        "labels = df['Label']\n",
        "AmazonLabels = amazon_df['Label']\n",
        "YelpLabels = yelp_df['Label']\n",
        "IMDBLabels = imdb_df['Label']\n",
        "\n",
        "#stored the labels in appropriate variables\n",
        "\n",
        "unigramTrain, unigramTest, unigramTrainLabels, unigramTestLabels = train_test_split(tfidfUniDF, labels)\n",
        "bigramTrain, bigramTest, bigramTrainLabels, bigramTestLabels = train_test_split(tfidfBiDF, labels)\n",
        "trigramTrain, trigramTest, trigramTrainLabels, trigramTestLabels = train_test_split(tfidfTriDF, labels)\n",
        "fourgramTrain, fourgramTest, fourgramTrainLabels, fourgramTestLabels = train_test_split(tfidfFourDF, labels)\n",
        "unigramAndbigramTrain, unigramAndbigramTest, unigramAndBigramTrainLabels, unigramAndBigramTestLabels = train_test_split(tfidfUniAndBiDF, labels)\n",
        "unigramBigramAndTrigramTrain, unigramBigramAndTrigramTest, unigramBigramAndTrigramTrainLabels, unigramBigramAndTrigramTestLabels = train_test_split(tfidfUniBiAndTriDF, labels)\n",
        "\n",
        "#above code performed train/test splits for entire compilation of reviews for each n-gram style\n",
        "\n",
        "AmazonunigramTrain, AmazonunigramTest, AmazonunigramTrainLabels, AmazonunigramTestLabels = train_test_split(AmazontfidfUniDF, AmazonLabels)\n",
        "AmazonbigramTrain, AmazonbigramTest, AmazonbigramTrainLabels, AmazonbigramTestLabels = train_test_split(AmazontfidfBiDF, AmazonLabels)\n",
        "AmazontrigramTrain, AmazontrigramTest, AmazontrigramTrainLabels, AmazontrigramTestLabels = train_test_split(AmazontfidfTriDF, AmazonLabels)\n",
        "AmazonfourgramTrain, AmazonfourgramTest, AmazonfourgramTrainLabels, AmazonfourgramTestLabels = train_test_split(AmazontfidfFourDF, AmazonLabels)\n",
        "AmazonunigramAndbigramTrain, AmazonunigramAndbigramTest, AmazonunigramAndBigramTrainLabels, AmazonunigramAndBigramTestLabels = train_test_split(AmazontfidfUniAndBiDF, AmazonLabels)\n",
        "AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTest, AmazonunigramBigramAndTrigramTrainLabels, AmazonunigramBigramAndTrigramTestLabels = train_test_split(AmazontfidfUniBiAndTriDF, AmazonLabels)\n",
        "#above code performed train/test splits for Amazon reviews for each n-gram style\n",
        "\n",
        "YelpunigramTrain, YelpunigramTest, YelpunigramTrainLabels, YelpunigramTestLabels = train_test_split(YelptfidfUniDF, YelpLabels)\n",
        "YelpbigramTrain, YelpbigramTest, YelpbigramTrainLabels, YelpbigramTestLabels = train_test_split(YelptfidfBiDF, YelpLabels)\n",
        "YelptrigramTrain, YelptrigramTest, YelptrigramTrainLabels, YelptrigramTestLabels = train_test_split(YelptfidfTriDF, YelpLabels)\n",
        "YelpfourgramTrain, YelpfourgramTest, YelpfourgramTrainLabels, YelpfourgramTestLabels = train_test_split(YelptfidfFourDF, YelpLabels)\n",
        "YelpunigramAndbigramTrain, YelpunigramAndbigramTest, YelpunigramAndBigramTrainLabels, YelpunigramAndBigramTestLabels = train_test_split(YelptfidfUniAndBiDF, YelpLabels)\n",
        "YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTest, YelpunigramBigramAndTrigramTrainLabels,YelpunigramBigramAndTrigramTestLabels = train_test_split(YelptfidfUniBiAndTriDF, YelpLabels)\n",
        "#above code performed train/test splits for Yelp reviews for each n-gram style\n",
        "\n",
        "IMDBunigramTrain, IMDBunigramTest, IMDBunigramTrainLabels, IMDBunigramTestLabels = train_test_split(IMDBtfidfUniDF, IMDBLabels)\n",
        "IMDBbigramTrain, IMDBbigramTest, IMDBbigramTrainLabels, IMDBbigramTestLabels = train_test_split(IMDBtfidfBiDF, IMDBLabels)\n",
        "IMDBtrigramTrain, IMDBtrigramTest, IMDBtrigramTrainLabels, IMDBtrigramTestLabels = train_test_split(IMDBtfidfTriDF, IMDBLabels)\n",
        "IMDBfourgramTrain, IMDBfourgramTest, IMDBfourgramTrainLabels, IMDBfourgramTestLabels = train_test_split(IMDBtfidfFourDF, IMDBLabels)\n",
        "IMDBunigramAndbigramTrain, IMDBunigramAndbigramTest, IMDBunigramAndBigramTrainLabels, IMDBunigramAndBigramTestLabels = train_test_split(IMDBtfidfUniAndBiDF, IMDBLabels)\n",
        "IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTest, IMDBunigramBigramAndTrigramTrainLabels,IMDBunigramBigramAndTrigramTestLabels = train_test_split(IMDBtfidfUniBiAndTriDF, IMDBLabels)\n",
        "#above code performed train/test splits for IMDB reviews for each n-gram style"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUQMbagan4yi"
      },
      "source": [
        "Baseline Naive Bayes Accuracy results on tfidf vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypf86TpKjus5",
        "outputId": "4b8347a1-f8e8-4dc5-8e1f-eff228c58c70"
      },
      "source": [
        "#on entire dataset\n",
        "uniNB = MultinomialNB()\n",
        "biNB = MultinomialNB()\n",
        "triNB = MultinomialNB()\n",
        "fourNB = MultinomialNB()\n",
        "uniAndbiNB = MultinomialNB()\n",
        "uniBiAndTriNB = MultinomialNB()\n",
        "\n",
        "uniNB.fit(unigramTrain, unigramTrainLabels)\n",
        "biNB.fit(bigramTrain, bigramTrainLabels)\n",
        "triNB.fit(trigramTrain, trigramTrainLabels)\n",
        "fourNB.fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndbiNB.fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriNB.fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "uniPreds = uniNB.predict(unigramTest)\n",
        "biPreds = biNB.predict(bigramTest)\n",
        "triPreds = triNB.predict(trigramTest)\n",
        "fourPreds = fourNB.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndbiNB.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriNB.predict(unigramBigramAndTrigramTest)\n",
        "\n",
        "uniNBAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biNBAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triNBAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourNBAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniAndBiNBAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriNBAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "#on Amazon\n",
        "AmazonuniNB = MultinomialNB()\n",
        "AmazonbiNB = MultinomialNB()\n",
        "AmazontriNB = MultinomialNB()\n",
        "AmazonfourNB = MultinomialNB()\n",
        "AmazonuniAndbiNB = MultinomialNB()\n",
        "AmazonuniBiAndTriNB = MultinomialNB()\n",
        "\n",
        "AmazonuniNB.fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiNB.fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriNB.fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourNB.fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndbiNB.fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriNB.fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniPreds = AmazonuniNB.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiNB.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriNB.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourNB.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndbiNB.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriNB.predict(AmazonunigramBigramAndTrigramTest)\n",
        "\n",
        "AmazonuniNBAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiNBAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriNBAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourNBAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniAndBiNBAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriNBAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "#on Yelp\n",
        "\n",
        "YelpuniNB = MultinomialNB()\n",
        "YelpbiNB = MultinomialNB()\n",
        "YelptriNB = MultinomialNB()\n",
        "YelpfourNB = MultinomialNB()\n",
        "YelpuniAndbiNB = MultinomialNB()\n",
        "YelpuniBiAndTriNB = MultinomialNB()\n",
        "\n",
        "YelpuniNB.fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiNB.fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriNB.fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourNB.fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndbiNB.fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriNB.fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniPreds =YelpuniNB.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiNB.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriNB.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourNB.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndbiNB.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriNB.predict(YelpunigramBigramAndTrigramTest)\n",
        "\n",
        "YelpuniNBAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiNBAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriNBAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourNBAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniAndBiNBAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriNBAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "#on IMDB\n",
        "\n",
        "IMDBuniNB = MultinomialNB()\n",
        "IMDBbiNB = MultinomialNB()\n",
        "IMDBtriNB = MultinomialNB()\n",
        "IMDBfourNB = MultinomialNB()\n",
        "IMDBuniAndbiNB = MultinomialNB()\n",
        "IMDBuniBiAndTriNB = MultinomialNB()\n",
        "\n",
        "IMDBuniNB.fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiNB.fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriNB.fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourNB.fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndbiNB.fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriNB.fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniPreds =IMDBuniNB.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiNB.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriNB.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourNB.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndbiNB.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriNB.predict(IMDBunigramBigramAndTrigramTest)\n",
        "\n",
        "IMDBuniNBAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiNBAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriNBAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourNBAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniAndBiNBAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriNBAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "print(\"NB trained on entire dataset...\\n\")\n",
        "print(\"unigram acc: \" + str(uniNBAcc))\n",
        "print(\"bigram acc: \"+str(biNBAcc))\n",
        "print(\"trigram acc: \" + str(triNBAcc))\n",
        "print(\"fourgram acc: \" + str(fourNBAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniAndBiNBAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriNBAcc))\n",
        "print(\"NB trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniNBAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiNBAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriNBAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourNBAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniAndBiNBAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriNBAcc))\n",
        "print(\"NB trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniNBAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiNBAcc))\n",
        "print(\"trigram acc: \" + str(YelptriNBAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourNBAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniAndBiNBAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriNBAcc))\n",
        "print(\"NB trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniNBAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiNBAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriNBAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourNBAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniAndBiNBAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriNBAcc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB trained on entire dataset...\n",
            "\n",
            "unigram acc: 0.8238719068413392\n",
            "bigram acc: 0.5749636098981077\n",
            "trigram acc: 0.5152838427947598\n",
            "fourgram acc: 0.5021834061135371\n",
            "uni and bigram acc: 0.7991266375545851\n",
            "uni bi and trigram acc: 0.8049490538573508\n",
            "NB trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.776\n",
            "bigram acc: 0.532\n",
            "trigram acc: 0.512\n",
            "fourgram acc: 0.472\n",
            "uni and bigram acc: 0.792\n",
            "uni bi and trigram acc: 0.808\n",
            "NB trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.768\n",
            "bigram acc: 0.564\n",
            "trigram acc: 0.472\n",
            "fourgram acc: 0.48\n",
            "uni and bigram acc: 0.78\n",
            "uni bi and trigram acc: 0.74\n",
            "NB trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7754010695187166\n",
            "bigram acc: 0.5828877005347594\n",
            "trigram acc: 0.49732620320855614\n",
            "fourgram acc: 0.48128342245989303\n",
            "uni and bigram acc: 0.7647058823529411\n",
            "uni bi and trigram acc: 0.7486631016042781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsiPdIkHeqtP"
      },
      "source": [
        "Logistic Regression for n-gram models in the cell directly below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63hJpEHev6z"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjjq4IESxOgx"
      },
      "source": [
        "Logistic Regression (C = 1; default; lower C indicates stronger regularization penalty)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orFpjl5juT5c",
        "outputId": "f4927068-764f-4411-e7c2-fe0ee30a6365"
      },
      "source": [
        "uniLR = LogisticRegression().fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression().fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression().fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression().fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression().fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression().fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression().fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression().fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression().fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression().fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression().fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression().fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression().fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression().fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression().fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression().fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression().fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression().fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression().fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression().fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression().fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression().fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression().fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression().fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset...\n",
            "\n",
            "unigram acc: 0.8049490538573508\n",
            "bigram acc: 0.5909752547307132\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5225618631732168\n",
            "uni and bigram acc: 0.7787481804949054\n",
            "uni bi and trigram acc: 0.8034934497816594\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.812\n",
            "bigram acc: 0.632\n",
            "trigram acc: 0.46\n",
            "fourgram acc: 0.508\n",
            "uni and bigram acc: 0.792\n",
            "uni bi and trigram acc: 0.744\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.712\n",
            "bigram acc: 0.536\n",
            "trigram acc: 0.476\n",
            "fourgram acc: 0.492\n",
            "uni and bigram acc: 0.752\n",
            "uni bi and trigram acc: 0.764\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7379679144385026\n",
            "bigram acc: 0.5347593582887701\n",
            "trigram acc: 0.48128342245989303\n",
            "fourgram acc: 0.5026737967914439\n",
            "uni and bigram acc: 0.7807486631016043\n",
            "uni bi and trigram acc: 0.6363636363636364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YArmbB0qxGzj"
      },
      "source": [
        "Logistic Regression (C = 0.9 where smaller C indicates stronger regularization penalty)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJBDG-TyxX2i",
        "outputId": "74712ed0-3ea7-43f2-9bee-00f014d94d1e"
      },
      "source": [
        "uniLR = LogisticRegression(C=0.9).fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression(C=0.9).fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression(C=0.9).fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression(C=0.9).fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression(C=0.9).fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression(C=0.9).fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression(C=0.9).fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression(C=0.9).fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression(C=0.9).fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression(C=0.9).fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression(C=0.9).fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression(C=0.9).fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression(C=0.9).fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression(C=0.9).fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression(C=0.9).fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression(C=0.9).fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression(C=0.9).fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression(C=0.9).fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression(C=0.9).fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression(C=0.9).fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression(C=0.9).fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression(C=0.9).fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression(C=0.9).fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression(C=0.9).fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset (C = 0.9)...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset (C = 0.9)...\n",
            "\n",
            "unigram acc: 0.8005822416302766\n",
            "bigram acc: 0.5909752547307132\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5225618631732168\n",
            "uni and bigram acc: 0.7802037845705968\n",
            "uni bi and trigram acc: 0.8034934497816594\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.812\n",
            "bigram acc: 0.628\n",
            "trigram acc: 0.46\n",
            "fourgram acc: 0.508\n",
            "uni and bigram acc: 0.792\n",
            "uni bi and trigram acc: 0.74\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.716\n",
            "bigram acc: 0.536\n",
            "trigram acc: 0.476\n",
            "fourgram acc: 0.492\n",
            "uni and bigram acc: 0.752\n",
            "uni bi and trigram acc: 0.76\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7219251336898396\n",
            "bigram acc: 0.5187165775401069\n",
            "trigram acc: 0.48128342245989303\n",
            "fourgram acc: 0.5026737967914439\n",
            "uni and bigram acc: 0.7807486631016043\n",
            "uni bi and trigram acc: 0.6310160427807486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvE0USCYx0gl"
      },
      "source": [
        "Logistic Regression (C=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNOqivAcx8_-",
        "outputId": "d8689c25-e27c-4bf8-a8c3-4f636717ca51"
      },
      "source": [
        "uniLR = LogisticRegression(C=0.8).fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression(C=0.8).fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression(C=0.8).fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression(C=0.8).fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression(C=0.8).fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression(C=0.8).fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression(C=0.8).fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression(C=0.8).fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression(C=0.8).fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression(C=0.8).fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression(C=0.8).fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression(C=0.8).fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression(C=0.8).fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression(C=0.8).fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression(C=0.8).fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression(C=0.8).fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression(C=0.8).fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression(C=0.8).fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression(C=0.8).fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression(C=0.8).fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression(C=0.8).fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression(C=0.8).fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression(C=0.8).fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression(C=0.8).fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset (C = 0.8)...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset (C = 0.8)...\n",
            "\n",
            "unigram acc: 0.8020378457059679\n",
            "bigram acc: 0.5909752547307132\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5225618631732168\n",
            "uni and bigram acc: 0.7802037845705968\n",
            "uni bi and trigram acc: 0.8034934497816594\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.812\n",
            "bigram acc: 0.628\n",
            "trigram acc: 0.46\n",
            "fourgram acc: 0.508\n",
            "uni and bigram acc: 0.792\n",
            "uni bi and trigram acc: 0.736\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.716\n",
            "bigram acc: 0.536\n",
            "trigram acc: 0.476\n",
            "fourgram acc: 0.492\n",
            "uni and bigram acc: 0.752\n",
            "uni bi and trigram acc: 0.756\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7165775401069518\n",
            "bigram acc: 0.5187165775401069\n",
            "trigram acc: 0.48128342245989303\n",
            "fourgram acc: 0.5026737967914439\n",
            "uni and bigram acc: 0.7754010695187166\n",
            "uni bi and trigram acc: 0.6149732620320856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-dH7Y3ByU5r"
      },
      "source": [
        "Logistic Regression (C = 0.6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyugyY2RyXvC",
        "outputId": "90b768a0-bd53-4646-ed41-b853578f84c1"
      },
      "source": [
        "uniLR = LogisticRegression(C=0.6).fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression(C=0.6).fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression(C=0.6).fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression(C=0.6).fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression(C=0.6).fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression(C=0.6).fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression(C=0.6).fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression(C=0.6).fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression(C=0.6).fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression(C=0.6).fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression(C=0.6).fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression(C=0.6).fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression(C=0.6).fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression(C=0.6).fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression(C=0.6).fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression(C=0.6).fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression(C=0.6).fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression(C=0.6).fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression(C=0.6).fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression(C=0.6).fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression(C=0.6).fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression(C=0.6).fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression(C=0.6).fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression(C=0.6).fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset (C = 0.6)...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset (C = 0.6)...\n",
            "\n",
            "unigram acc: 0.7991266375545851\n",
            "bigram acc: 0.5938864628820961\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5225618631732168\n",
            "uni and bigram acc: 0.7787481804949054\n",
            "uni bi and trigram acc: 0.7976710334788938\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.796\n",
            "bigram acc: 0.608\n",
            "trigram acc: 0.456\n",
            "fourgram acc: 0.492\n",
            "uni and bigram acc: 0.792\n",
            "uni bi and trigram acc: 0.732\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.704\n",
            "bigram acc: 0.524\n",
            "trigram acc: 0.476\n",
            "fourgram acc: 0.492\n",
            "uni and bigram acc: 0.744\n",
            "uni bi and trigram acc: 0.744\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7165775401069518\n",
            "bigram acc: 0.5133689839572193\n",
            "trigram acc: 0.48128342245989303\n",
            "fourgram acc: 0.5026737967914439\n",
            "uni and bigram acc: 0.7540106951871658\n",
            "uni bi and trigram acc: 0.5935828877005348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8JBgnV99Q0"
      },
      "source": [
        "C = 1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWy6O6nH9-_B",
        "outputId": "a08c7e5a-5434-4adc-e3c0-76e325c00c30"
      },
      "source": [
        "uniLR = LogisticRegression(C=1.3).fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression(C=1.3).fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression(C=1.3).fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression(C=1.3).fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression(C=1.3).fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression(C=1.3).fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression(C=1.3).fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression(C=1.3).fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression(C=1.3).fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression(C=1.3).fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression(C=1.3).fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression(C=1.3).fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression(C=1.3).fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression(C=1.3).fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression(C=1.3).fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression(C=1.3).fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression(C=1.3).fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression(C=1.3).fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression(C=1.3).fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression(C=1.3).fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression(C=1.3).fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression(C=1.3).fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression(C=1.3).fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression(C=1.3).fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset (C = 1.3)...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset (C = 1.3)...\n",
            "\n",
            "unigram acc: 0.8282387190684134\n",
            "bigram acc: 0.5749636098981077\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5007278020378457\n",
            "uni and bigram acc: 0.8136826783114993\n",
            "uni bi and trigram acc: 0.7947598253275109\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.784\n",
            "bigram acc: 0.644\n",
            "trigram acc: 0.492\n",
            "fourgram acc: 0.472\n",
            "uni and bigram acc: 0.78\n",
            "uni bi and trigram acc: 0.796\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.788\n",
            "bigram acc: 0.568\n",
            "trigram acc: 0.492\n",
            "fourgram acc: 0.48\n",
            "uni and bigram acc: 0.808\n",
            "uni bi and trigram acc: 0.752\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7486631016042781\n",
            "bigram acc: 0.5614973262032086\n",
            "trigram acc: 0.5508021390374331\n",
            "fourgram acc: 0.47593582887700536\n",
            "uni and bigram acc: 0.7112299465240641\n",
            "uni bi and trigram acc: 0.7486631016042781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1hAuYxZ_PvZ"
      },
      "source": [
        "C = 1.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I4bu1_u_Q_J",
        "outputId": "3642d7c2-c34f-458a-a462-18216493700f"
      },
      "source": [
        "uniLR = LogisticRegression(C=1.5).fit(unigramTrain, unigramTrainLabels)\n",
        "biLR = LogisticRegression(C=1.5).fit(bigramTrain, bigramTrainLabels)\n",
        "triLR = LogisticRegression(C=1.5).fit(trigramTrain, trigramTrainLabels)\n",
        "fourLR = LogisticRegression(C=1.5).fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiLR = LogisticRegression(C=1.5).fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriLR = LogisticRegression(C=1.5).fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "AmazonuniLR = LogisticRegression(C=1.5).fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiLR = LogisticRegression(C=1.5).fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriLR = LogisticRegression(C=1.5).fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourLR = LogisticRegression(C=1.5).fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiLR = LogisticRegression(C=1.5).fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriLR = LogisticRegression(C=1.5).fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "YelpuniLR = LogisticRegression(C=1.5).fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiLR = LogisticRegression(C=1.5).fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriLR = LogisticRegression(C=1.5).fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourLR = LogisticRegression(C=1.5).fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiLR = LogisticRegression(C=1.5).fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriLR = LogisticRegression(C=1.5).fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "IMDBuniLR = LogisticRegression(C=1.5).fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiLR = LogisticRegression(C=1.5).fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriLR = LogisticRegression(C=1.5).fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourLR = LogisticRegression(C=1.5).fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiLR = LogisticRegression(C=1.5).fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriLR = LogisticRegression(C=1.5).fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniLR.predict(unigramTest)\n",
        "biPreds = biLR.predict(bigramTest)\n",
        "triPreds = triLR.predict(trigramTest)\n",
        "fourPreds = fourLR.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiLR.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriLR.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniLR.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiLR.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriLR.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourLR.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiLR.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriLR.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniLR.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiLR.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriLR.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourLR.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiLR.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriLR.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniLR.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiLR.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriLR.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourLR.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiLR.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriLR.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"LR trained on entire dataset (C = 1.5)...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"LR trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"LR trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"LR trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR trained on entire dataset (C = 1.5)...\n",
            "\n",
            "unigram acc: 0.8253275109170306\n",
            "bigram acc: 0.5720524017467249\n",
            "trigram acc: 0.5254730713245997\n",
            "fourgram acc: 0.5007278020378457\n",
            "uni and bigram acc: 0.8107714701601164\n",
            "uni bi and trigram acc: 0.7976710334788938\n",
            "LR trained on Amazon ...\n",
            "\n",
            "unigram acc: 0.788\n",
            "bigram acc: 0.644\n",
            "trigram acc: 0.492\n",
            "fourgram acc: 0.472\n",
            "uni and bigram acc: 0.784\n",
            "uni bi and trigram acc: 0.8\n",
            "LR trained on Yelp ...\n",
            "\n",
            "unigram acc: 0.788\n",
            "bigram acc: 0.568\n",
            "trigram acc: 0.492\n",
            "fourgram acc: 0.48\n",
            "uni and bigram acc: 0.808\n",
            "uni bi and trigram acc: 0.744\n",
            "LR trained on IMDB .. \n",
            "\n",
            "unigram acc: 0.7540106951871658\n",
            "bigram acc: 0.5614973262032086\n",
            "trigram acc: 0.5508021390374331\n",
            "fourgram acc: 0.47593582887700536\n",
            "uni and bigram acc: 0.7165775401069518\n",
            "uni bi and trigram acc: 0.7540106951871658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd_aW1yAdy7s"
      },
      "source": [
        "SVM on BOW/tfidf vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl2f4Q9IPND2"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlpHUtx82WBz"
      },
      "source": [
        "C = 1 (C is inversely proportional to regularization strength)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVX25rPm6B9K"
      },
      "source": [
        "uniSVM = SVC().fit(unigramTrain, unigramTrainLabels)\n",
        "biSVM = SVC().fit(bigramTrain, bigramTrainLabels)\n",
        "triSVM = SVC().fit(trigramTrain, trigramTrainLabels)\n",
        "fourSVM = SVC().fit(fourgramTrain, fourgramTrainLabels)\n",
        "uniAndBiSVM = SVC().fit(unigramAndbigramTrain, unigramAndBigramTrainLabels)\n",
        "uniBiAndTriSVM = SVC().fit(unigramBigramAndTrigramTrain, unigramBigramAndTrigramTrainLabels)\n",
        "print(\"entire done\\n\")\n",
        "AmazonuniSVM = SVC().fit(AmazonunigramTrain, AmazonunigramTrainLabels)\n",
        "AmazonbiSVM = SVC().fit(AmazonbigramTrain, AmazonbigramTrainLabels)\n",
        "AmazontriSVM = SVC().fit(AmazontrigramTrain, AmazontrigramTrainLabels)\n",
        "AmazonfourSVM = SVC().fit(AmazonfourgramTrain, AmazonfourgramTrainLabels)\n",
        "AmazonuniAndBiSVM = SVC().fit(AmazonunigramAndbigramTrain, AmazonunigramAndBigramTrainLabels)\n",
        "AmazonuniBiAndTriSVM = SVC().fit(AmazonunigramBigramAndTrigramTrain, AmazonunigramBigramAndTrigramTrainLabels)\n",
        "print(\"amazon done\\n\")\n",
        "YelpuniSVM = SVC().fit(YelpunigramTrain, YelpunigramTrainLabels)\n",
        "YelpbiSVM = SVC().fit(YelpbigramTrain, YelpbigramTrainLabels)\n",
        "YelptriSVM = SVC().fit(YelptrigramTrain, YelptrigramTrainLabels)\n",
        "YelpfourSVM = SVC().fit(YelpfourgramTrain, YelpfourgramTrainLabels)\n",
        "YelpuniAndBiSVM = SVC().fit(YelpunigramAndbigramTrain, YelpunigramAndBigramTrainLabels)\n",
        "YelpuniBiAndTriSVM = SVC().fit(YelpunigramBigramAndTrigramTrain, YelpunigramBigramAndTrigramTrainLabels)\n",
        "print(\"yelp done\\n\")\n",
        "IMDBuniSVM = SVC().fit(IMDBunigramTrain, IMDBunigramTrainLabels)\n",
        "IMDBbiSVM = SVC().fit(IMDBbigramTrain, IMDBbigramTrainLabels)\n",
        "IMDBtriSVM = SVC().fit(IMDBtrigramTrain, IMDBtrigramTrainLabels)\n",
        "IMDBfourSVM = SVC().fit(IMDBfourgramTrain, IMDBfourgramTrainLabels)\n",
        "IMDBuniAndBiSVM = SVC().fit(IMDBunigramAndbigramTrain, IMDBunigramAndBigramTrainLabels)\n",
        "IMDBuniBiAndTriSVM = SVC().fit(IMDBunigramBigramAndTrigramTrain, IMDBunigramBigramAndTrigramTrainLabels)\n",
        "\n",
        "\n",
        "\n",
        "uniPreds = uniSVM.predict(unigramTest)\n",
        "biPreds = biSVM.predict(bigramTest)\n",
        "triPreds = triSVM.predict(trigramTest)\n",
        "fourPreds = fourSVM.predict(fourgramTest)\n",
        "uniAndBiPreds = uniAndBiSVM.predict(unigramAndbigramTest)\n",
        "uniBiAndTriPreds = uniBiAndTriSVM.predict(unigramBigramAndTrigramTest)\n",
        "uniAcc = np.mean(uniPreds==unigramTestLabels)\n",
        "biAcc = np.mean(biPreds==bigramTestLabels)\n",
        "triAcc = np.mean(triPreds==trigramTestLabels)\n",
        "fourAcc = np.mean(fourPreds==fourgramTestLabels)\n",
        "uniBiAcc = np.mean(uniAndBiPreds==unigramAndBigramTestLabels)\n",
        "uniBiAndTriAcc = np.mean(uniBiAndTriPreds==unigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "\n",
        "AmazonuniPreds = AmazonuniSVM.predict(AmazonunigramTest)\n",
        "AmazonbiPreds = AmazonbiSVM.predict(AmazonbigramTest)\n",
        "AmazontriPreds = AmazontriSVM.predict(AmazontrigramTest)\n",
        "AmazonfourPreds = AmazonfourSVM.predict(AmazonfourgramTest)\n",
        "AmazonuniAndBiPreds = AmazonuniAndBiSVM.predict(AmazonunigramAndbigramTest)\n",
        "AmazonuniBiAndTriPreds = AmazonuniBiAndTriSVM.predict(AmazonunigramBigramAndTrigramTest)\n",
        "AmazonuniAcc = np.mean(AmazonuniPreds==AmazonunigramTestLabels)\n",
        "AmazonbiAcc = np.mean(AmazonbiPreds==AmazonbigramTestLabels)\n",
        "AmazontriAcc = np.mean(AmazontriPreds==AmazontrigramTestLabels)\n",
        "AmazonfourAcc = np.mean(AmazonfourPreds==AmazonfourgramTestLabels)\n",
        "AmazonuniBiAcc = np.mean(AmazonuniAndBiPreds==AmazonunigramAndBigramTestLabels)\n",
        "AmazonuniBiAndTriAcc = np.mean(AmazonuniBiAndTriPreds==AmazonunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "YelpuniPreds = YelpuniSVM.predict(YelpunigramTest)\n",
        "YelpbiPreds = YelpbiSVM.predict(YelpbigramTest)\n",
        "YelptriPreds = YelptriSVM.predict(YelptrigramTest)\n",
        "YelpfourPreds = YelpfourSVM.predict(YelpfourgramTest)\n",
        "YelpuniAndBiPreds = YelpuniAndBiSVM.predict(YelpunigramAndbigramTest)\n",
        "YelpuniBiAndTriPreds = YelpuniBiAndTriSVM.predict(YelpunigramBigramAndTrigramTest)\n",
        "YelpuniAcc = np.mean(YelpuniPreds==YelpunigramTestLabels)\n",
        "YelpbiAcc = np.mean(YelpbiPreds==YelpbigramTestLabels)\n",
        "YelptriAcc = np.mean(YelptriPreds==YelptrigramTestLabels)\n",
        "YelpfourAcc = np.mean(YelpfourPreds==YelpfourgramTestLabels)\n",
        "YelpuniBiAcc = np.mean(YelpuniAndBiPreds==YelpunigramAndBigramTestLabels)\n",
        "YelpuniBiAndTriAcc = np.mean(YelpuniBiAndTriPreds==YelpunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "IMDBuniPreds = IMDBuniSVM.predict(IMDBunigramTest)\n",
        "IMDBbiPreds = IMDBbiSVM.predict(IMDBbigramTest)\n",
        "IMDBtriPreds = IMDBtriSVM.predict(IMDBtrigramTest)\n",
        "IMDBfourPreds = IMDBfourSVM.predict(IMDBfourgramTest)\n",
        "IMDBuniAndBiPreds = IMDBuniAndBiSVM.predict(IMDBunigramAndbigramTest)\n",
        "IMDBuniBiAndTriPreds = IMDBuniBiAndTriSVM.predict(IMDBunigramBigramAndTrigramTest)\n",
        "IMDBuniAcc = np.mean(IMDBuniPreds==IMDBunigramTestLabels)\n",
        "IMDBbiAcc = np.mean(IMDBbiPreds==IMDBbigramTestLabels)\n",
        "IMDBtriAcc = np.mean(IMDBtriPreds==IMDBtrigramTestLabels)\n",
        "IMDBfourAcc = np.mean(IMDBfourPreds==IMDBfourgramTestLabels)\n",
        "IMDBuniBiAcc = np.mean(IMDBuniAndBiPreds==IMDBunigramAndBigramTestLabels)\n",
        "IMDBuniBiAndTriAcc = np.mean(IMDBuniBiAndTriPreds==IMDBunigramBigramAndTrigramTestLabels)\n",
        "\n",
        "\n",
        "print(\"SVM trained on entire dataset...\\n\")\n",
        "print(\"unigram acc: \" + str(uniAcc))\n",
        "print(\"bigram acc: \"+str(biAcc))\n",
        "print(\"trigram acc: \" + str(triAcc))\n",
        "print(\"fourgram acc: \" + str(fourAcc))\n",
        "print(\"uni and bigram acc: \" + str(uniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(uniBiAndTriAcc))\n",
        "print(\"SVM trained on Amazon ...\\n\")\n",
        "print(\"unigram acc: \" + str(AmazonuniAcc))\n",
        "print(\"bigram acc: \"+str(AmazonbiAcc))\n",
        "print(\"trigram acc: \" + str(AmazontriAcc))\n",
        "print(\"fourgram acc: \" + str(AmazonfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(AmazonuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(AmazonuniBiAndTriAcc))\n",
        "print(\"SVM trained on Yelp ...\\n\")\n",
        "print(\"unigram acc: \" + str(YelpuniAcc))\n",
        "print(\"bigram acc: \"+str(YelpbiAcc))\n",
        "print(\"trigram acc: \" + str(YelptriAcc))\n",
        "print(\"fourgram acc: \" + str(YelpfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(YelpuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(YelpuniBiAndTriAcc))\n",
        "print(\"SVM trained on IMDB .. \\n\")\n",
        "print(\"unigram acc: \" + str(IMDBuniAcc))\n",
        "print(\"bigram acc: \"+str(IMDBbiAcc))\n",
        "print(\"trigram acc: \" + str(IMDBtriAcc))\n",
        "print(\"fourgram acc: \" + str(IMDBfourAcc))\n",
        "print(\"uni and bigram acc: \" + str(IMDBuniBiAcc))\n",
        "print(\"uni bi and trigram acc: \" + str(IMDBuniBiAndTriAcc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T78hC2A-HAkZ"
      },
      "source": [
        "Explore Word2Vec\n",
        "\n",
        "Word2Vec will enable us to find a vector representation of each word but our reviews and tweets will be of different lengths. Our idea is to visualize the distribution of review lengths and then set an appropriate cap on the length of the review. Reviews under this length will be padded with 0s and reviews over this length will be cut. Then, once all reviews are of the same length (ie: same number of words), we can use Word2Vec to get a fixed length numeric representation for each word which implies a fixed length numeric representation for each review. If a word in the corpus doens't occur in the Word2Vec library, toss the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "pmm1jLEMG_cg",
        "outputId": "9beb3692-0bd8-42e1-bbb5-c305bc07f652"
      },
      "source": [
        "lengths = []\n",
        "sents = list(df['Sentence'])\n",
        "for i in range(len(sents)):\n",
        "  s = sents[i]\n",
        "  lengths.append(len(s.split()))\n",
        "lengths = np.array(lengths)\n",
        "from matplotlib import pyplot as plt\n",
        "plt.hist(lengths,range=(0,100), bins=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 97., 739., 631., 479., 316., 215., 120.,  71.,  30.,  14.,  14.,\n",
              "          4.,   4.,   2.,   3.,   1.,   0.,   1.,   1.,   0.,   1.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.]),\n",
              " array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,\n",
              "         22.,  24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,\n",
              "         44.,  46.,  48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,\n",
              "         66.,  68.,  70.,  72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,\n",
              "         88.,  90.,  92.,  94.,  96.,  98., 100.]),\n",
              " <a list of 50 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARM0lEQVR4nO3df4xlZX3H8fenrPgDW5cf0w3d3XRo3GhIE4FO6BqNsWxtBIzLH0owpmzIJts/aKvVxK7tH41J/1iSRpSkIdmIuhirItWyAWJLV4zpH6CDUkRWy0jB3c3CjgrrD2KV+u0f99l6WWeYOzN3Ztxn36/k5j7ne54z5zk58Nkzzz33TKoKSVJffmOtByBJGj/DXZI6ZLhLUocMd0nqkOEuSR1at9YDADjvvPNqcnJyrYchSaeUBx544HtVNTHXul+LcJ+cnGR6enqthyFJp5QkT8y3zmkZSeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nq0K/FN1RXwuTuu+asP77nylUeiSStPq/cJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocWDPckr0ry4NDrh0neneScJPckebS9n936J8lNSWaSPJTkkpU/DEnSsAXDvaq+XVUXVdVFwB8AzwKfB3YDB6pqC3CgLQNcDmxpr13AzSsxcEnS/BY7LbMN+E5VPQFsB/a1+j7gqtbeDtxaA/cB65OcP5bRSpJGsthwvwb4VGtvqKqjrf0ksKG1NwKHhrY53GqSpFUycrgnORN4K/DZk9dVVQG1mB0n2ZVkOsn07OzsYjaVJC1gMVfulwNfq6qn2vJTJ6Zb2vuxVj8CbB7ablOrPU9V7a2qqaqampiYWPzIJUnzWky4v4NfTskA7Ad2tPYO4I6h+rXtrpmtwPGh6RtJ0ioY6amQSc4C3gT82VB5D3Bbkp3AE8DVrX43cAUww+DOmuvGNlpJ0khGCveq+glw7km17zO4e+bkvgVcP5bRSZKWxG+oSlKHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOjfTI355M7r5r3nWP77lyFUciSSvHK3dJ6pDhLkkdMtwlqUOGuyR1yHCXpA6NFO5J1ie5Pcm3khxM8tok5yS5J8mj7f3s1jdJbkoyk+ShJJes7CFIkk426pX7h4EvVNWrgdcAB4HdwIGq2gIcaMsAlwNb2msXcPNYRyxJWtCC4Z7kFcAbgFsAqupnVfUMsB3Y17rtA65q7e3ArTVwH7A+yfljH7kkaV6jXLlfAMwCH0vy9SQfSXIWsKGqjrY+TwIbWnsjcGho+8Ot9jxJdiWZTjI9Ozu79COQJP2KUcJ9HXAJcHNVXQz8hF9OwQBQVQXUYnZcVXuraqqqpiYmJhazqSRpAaOE+2HgcFXd35ZvZxD2T52Ybmnvx9r6I8Dmoe03tZokaZUsGO5V9SRwKMmrWmkb8AiwH9jRajuAO1p7P3Btu2tmK3B8aPpGkrQKRn1w2F8An0xyJvAYcB2DfxhuS7ITeAK4uvW9G7gCmAGebX0lSatopHCvqgeBqTlWbZujbwHXL3NckqRl8BuqktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUodGCvckjyf5RpIHk0y32jlJ7knyaHs/u9WT5KYkM0keSnLJSh6AJOlXLebK/Y+q6qKqOvGHsncDB6pqC3CgLQNcDmxpr13AzeMarCRpNMuZltkO7GvtfcBVQ/Vba+A+YH2S85exH0nSIo0a7gX8W5IHkuxqtQ1VdbS1nwQ2tPZG4NDQtodb7XmS7EoynWR6dnZ2CUOXJM1n3Yj9Xl9VR5L8NnBPkm8Nr6yqSlKL2XFV7QX2AkxNTS1qW0nSCxvpyr2qjrT3Y8DngUuBp05Mt7T3Y637EWDz0OabWk2StEoWDPckZyX5zRNt4E+Ah4H9wI7WbQdwR2vvB65td81sBY4PTd9IklbBKNMyG4DPJznR/5+q6gtJvgrclmQn8ARwdet/N3AFMAM8C1w39lFLkl7QguFeVY8Br5mj/n1g2xz1Aq4fy+gkSUsy6geqp4XJ3XfNWX98z5WrPBJJWh4fPyBJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUMjh3uSM5J8PcmdbfmCJPcnmUnymSRntvqL2/JMWz+5MkOXJM1nMVfu7wIODi3fANxYVa8EngZ2tvpO4OlWv7H1kyStopHCPckm4ErgI205wGXA7a3LPuCq1t7elmnrt7X+kqRVMuqV+4eA9wG/aMvnAs9U1XNt+TCwsbU3AocA2vrjrf/zJNmVZDrJ9Ozs7BKHL0may4LhnuQtwLGqemCcO66qvVU1VVVTExMT4/zRknTaWzdCn9cBb01yBfAS4LeADwPrk6xrV+ebgCOt/xFgM3A4yTrgFcD3xz5ySdK8Frxyr6r3V9WmqpoErgG+WFXvBO4F3ta67QDuaO39bZm2/otVVWMdtSTpBS3nPve/Bt6TZIbBnPotrX4LcG6rvwfYvbwhSpIWa5Rpmf9XVV8CvtTajwGXztHnp8DbxzA2SdIS+Q1VSeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6tCi/hLT6Wpy911z1h/fc+Uqj0SSRuOVuyR1aMFwT/KSJF9J8p9JvpnkA61+QZL7k8wk+UySM1v9xW15pq2fXNlDkCSdbJQr9/8BLquq1wAXAW9OshW4Abixql4JPA3sbP13Ak+3+o2tnyRpFS0Y7jXw47b4ovYq4DLg9lbfB1zV2tvbMm39tiQZ24glSQsaac49yRlJHgSOAfcA3wGeqarnWpfDwMbW3ggcAmjrjwPnzvEzdyWZTjI9Ozu7vKOQJD3PSOFeVf9bVRcBm4BLgVcvd8dVtbeqpqpqamJiYrk/TpI0ZFF3y1TVM8C9wGuB9UlO3Eq5CTjS2keAzQBt/SuA749ltJKkkYxyt8xEkvWt/VLgTcBBBiH/ttZtB3BHa+9vy7T1X6yqGuegJUkvbJQvMZ0P7EtyBoN/DG6rqjuTPAJ8OsnfA18Hbmn9bwE+kWQG+AFwzQqMW5L0AhYM96p6CLh4jvpjDObfT67/FHj7WEYnSVoSv6EqSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDo/wNVc1jcvddc9Yf33PlKo9Ekp5vwSv3JJuT3JvkkSTfTPKuVj8nyT1JHm3vZ7d6ktyUZCbJQ0kuWemDkCQ93yjTMs8B762qC4GtwPVJLgR2AweqagtwoC0DXA5saa9dwM1jH7Uk6QUtGO5VdbSqvtbaPwIOAhuB7cC+1m0fcFVrbwdurYH7gPVJzh/7yCVJ81rUB6pJJoGLgfuBDVV1tK16EtjQ2huBQ0ObHW61k3/WriTTSaZnZ2cXOWxJ0gsZOdyTvBz4Z+DdVfXD4XVVVUAtZsdVtbeqpqpqamJiYjGbSpIWMFK4J3kRg2D/ZFV9rpWfOjHd0t6PtfoRYPPQ5ptaTZK0Ska5WybALcDBqvrg0Kr9wI7W3gHcMVS/tt01sxU4PjR9I0laBaPc5/464E+BbyR5sNX+BtgD3JZkJ/AEcHVbdzdwBTADPAtcN9YRS5IWtGC4V9V/AJln9bY5+hdw/TLHJUlaBh8/IEkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nq0Ch/iUmLNLn7rjnrj++5cpVHIul05ZW7JHXIcJekDi0Y7kk+muRYkoeHauckuSfJo+397FZPkpuSzCR5KMklKzl4SdLcRrly/zjw5pNqu4EDVbUFONCWAS4HtrTXLuDm8QxTkrQYC4Z7VX0Z+MFJ5e3AvtbeB1w1VL+1Bu4D1ic5f1yDlSSNZqlz7huq6mhrPwlsaO2NwKGhfodb7Vck2ZVkOsn07OzsEochSZrLsm+FrKpKUkvYbi+wF2BqamrR258w322HknQ6W+qV+1Mnplva+7FWPwJsHuq3qdUkSatoqeG+H9jR2juAO4bq17a7ZrYCx4embyRJq2TBaZkknwLeCJyX5DDwd8Ae4LYkO4EngKtb97uBK4AZ4FnguhUYsyRpAQuGe1W9Y55V2+boW8D1yx2UJGl5/IaqJHXIcJekDvlUyFXk0yIlrRav3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yMcP/BrwsQSSxs0rd0nqkOEuSR0y3CWpQ4a7JHXID1R/jc33Qet8/ABW0gkrcuWe5M1Jvp1kJsnuldiHJGl+Yw/3JGcA/whcDlwIvCPJhePejyRpfisxLXMpMFNVjwEk+TSwHXhkBfalIYudxlkKp36kU8NKhPtG4NDQ8mHgD0/ulGQXsKst/jjJt5e4v/OA7y1x21PVmh1zbliLvQKe59OFx7w4vzvfijX7QLWq9gJ7l/tzkkxX1dQYhnTK8JhPDx7z6WGljnklPlA9AmweWt7UapKkVbIS4f5VYEuSC5KcCVwD7F+B/UiS5jH2aZmqei7JnwP/CpwBfLSqvjnu/QxZ9tTOKchjPj14zKeHFTnmVNVK/FxJ0hry8QOS1CHDXZI6dEqH++nwmIMkm5Pcm+SRJN9M8q5WPyfJPUkebe9nr/VYxynJGUm+nuTOtnxBkvvbuf5M+7C+G0nWJ7k9ybeSHEzy2tPgHP9V+2/64SSfSvKS3s5zko8mOZbk4aHanOc1Aze1Y38oySXL2fcpG+6n0WMOngPeW1UXAluB69tx7gYOVNUW4EBb7sm7gINDyzcAN1bVK4GngZ1rMqqV82HgC1X1auA1DI6923OcZCPwl8BUVf0+g5svrqG/8/xx4M0n1eY7r5cDW9prF3DzcnZ8yoY7Q485qKqfAScec9CVqjpaVV9r7R8x+J9+I4Nj3de67QOuWpsRjl+STcCVwEfacoDLgNtbl96O9xXAG4BbAKrqZ1X1DB2f42Yd8NIk64CXAUfp7DxX1ZeBH5xUnu+8bgdurYH7gPVJzl/qvk/lcJ/rMQcb12gsqyLJJHAxcD+woaqOtlVPAhvWaFgr4UPA+4BftOVzgWeq6rm23Nu5vgCYBT7WpqI+kuQsOj7HVXUE+AfguwxC/TjwAH2f5xPmO69jzbRTOdxPK0leDvwz8O6q+uHwuhrcz9rFPa1J3gIcq6oH1nosq2gdcAlwc1VdDPyEk6ZgejrHAG2eeTuDf9h+BziLX52+6N5KntdTOdxPm8ccJHkRg2D/ZFV9rpWfOvErW3s/tlbjG7PXAW9N8jiDqbbLGMxHr2+/vkN/5/owcLiq7m/LtzMI+17PMcAfA/9dVbNV9XPgcwzOfc/n+YT5zutYM+1UDvfT4jEHbb75FuBgVX1waNV+YEdr7wDuWO2xrYSqen9VbaqqSQbn9ItV9U7gXuBtrVs3xwtQVU8Ch5K8qpW2MXhEdpfnuPkusDXJy9p/4yeOudvzPGS+87ofuLbdNbMVOD40fbN4VXXKvoArgP8CvgP87VqPZ4WO8fUMfm17CHiwva5gMA99AHgU+HfgnLUe6woc+xuBO1v794CvADPAZ4EXr/X4xnysFwHT7Tz/C3B27+cY+ADwLeBh4BPAi3s7z8CnGHym8HMGv6HtnO+8AmFwB+B3gG8wuJNoyfv28QOS1KFTeVpGkjQPw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR16P8A4yduUaDLA2MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC2cZ7YrJSRG"
      },
      "source": [
        "It looks like nearly all reviews are less than 20 words so let's cap our length at that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV07sNdvIfcx"
      },
      "source": [
        "import warnings \n",
        "  \n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "  \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "\n",
        "ListOfList = []\n",
        "for i in range(len(sents)):\n",
        "  s = sents[i]\n",
        "  ListOfList.append(s.split())\n",
        "\n",
        "word2vecModel = gensim.models.Word2Vec(ListOfList,min_count=1,size=60, window=5)\n",
        "\n",
        "# word embeddings of size 30, reviews being capped at length 20 so total size of review embedding => 60 x 20 => 1200\n",
        "Allw2v = np.zeros((df.shape[0],1200))\n",
        "\n",
        "for i in range(len(ListOfList)):\n",
        "  s = ListOfList[i] #s = list of words in sentence\n",
        "  if len(s) < 20:\n",
        "    embdList = []\n",
        "    for j in range(len(s)):\n",
        "      word = s[j]\n",
        "      try:\n",
        "        embdList+=list(word2vecModel.wv[word].reshape(-1))\n",
        "      except KeyError:\n",
        "        print(\"ignoring word..\")\n",
        "    embdList = np.array(embdList).reshape(-1)\n",
        "    Allw2v[i, 0:embdList.shape[0]] = embdList\n",
        "  else:\n",
        "    s = s[:20]\n",
        "    embdList = []\n",
        "    for j in range(len(s)):\n",
        "      word = s[j]\n",
        "      try:\n",
        "        embdList+=list(word2vecModel.wv[word].reshape(-1))\n",
        "      except KeyError:\n",
        "        print(\"ignoring word..\")\n",
        "    embdList = np.array(embdList).reshape(-1)\n",
        "    Allw2v[i, 0:embdList.shape[0]] = embdList\n",
        "\n",
        "index = [num for num in range(Allw2v.shape[0])]\n",
        "columns = [num for num in range(Allw2v.shape[1])]\n",
        "Allw2vDF = pd.DataFrame(data=Allw2v,index = index, columns=columns)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkuDKCJ9OusM"
      },
      "source": [
        "Allw2vDF is now a dataframe of the overall review datasets where the first 20 words are converted into Word2Vec embeddings of size 30. Note, the rightmost column of the df are the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8UJdEd1O9Ll",
        "outputId": "1ea51805-be60-4f1a-803e-9928a89e9149"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels = df['Label']\n",
        "w2vTrain, w2vTest, w2vTrainLabels, w2vTestLabels = train_test_split(Allw2vDF,labels)  #train-test split \n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB().fit(w2vTrain, w2vTrainLabels)\n",
        "nbPreds =nb.predict(w2vTest)\n",
        "nbAcc = np.mean(nbPreds==w2vTestLabels)\n",
        "\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression().fit(w2vTrain, w2vTrainLabels)\n",
        "lrPreds = lr.predict(w2vTest)\n",
        "lrAcc = np.mean(lrPreds==w2vTestLabels)\n",
        "\n",
        "#SVM\n",
        "svm = SVC().fit(w2vTrain, w2vTrainLabels)\n",
        "svmPreds = svm.predict(w2vTest)\n",
        "svmAcc = np.mean(svmPreds==w2vTestLabels)\n",
        "\n",
        "print(\"Word2Vec embedding results ..\\n\")\n",
        "print(\"Naive Bayes Acc: \" + str(nbAcc)+\"\\n\")\n",
        "print(\"LR Acc: \" + str(lrAcc)+\"\\n\")\n",
        "print(\"SVM Acc: \" + str(svmAcc))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec embedding results ..\n",
            "\n",
            "Naive Bayes Acc: 0.5240174672489083\n",
            "\n",
            "LR Acc: 0.4847161572052402\n",
            "\n",
            "SVM Acc: 0.5851528384279476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0pjZZ7bZlwX"
      },
      "source": [
        "Note: Word2Vec above is trained with OUR corpus and not pre-trained which might be a cause of the poor performance. So, let's convert our sentences to numerical vectors using a pre-trained word2vec lib now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwz3JDgoTB2Y",
        "outputId": "c61ee31c-e82a-4180-9c4d-d002689131f5"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.3% 1617.6/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfAr56C3dMFR"
      },
      "source": [
        "Loaded in word2vec trained on Google News dataset. Word Embeddings live in R^300 space so if we only care about the first 20 words of a review then we have 300*20 = 6000 features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk4feoWXdBtm",
        "outputId": "2a4bffc3-2a44-4ddc-a403-c6aa82043bb3"
      },
      "source": [
        "ListOfList = []\n",
        "sents = list(df['Sentence'])\n",
        "for i in range(len(sents)):\n",
        "  s = sents[i]\n",
        "  ListOfList.append(s.split())\n",
        "\n",
        "Allw2v = np.zeros((df.shape[0],6000))\n",
        "\n",
        "for i in range(len(ListOfList)):\n",
        "  if (i%100==0):\n",
        "    print(i)\n",
        "  s = ListOfList[i] #s = list of words in sentence\n",
        "  rem = []\n",
        "  for l in range(len(s)):\n",
        "    word = s[l]\n",
        "    if word not in wv.index2word:\n",
        "      rem.append(l)\n",
        "  temp = s\n",
        "  s = []\n",
        "  for l in range(len(temp)):\n",
        "    if l not in rem:\n",
        "      s.append(temp[l])\n",
        "  if len(s) < 20:\n",
        "    embdList = []\n",
        "    for j in range(len(s)):\n",
        "      word = s[j]\n",
        "      vec = wv[word]\n",
        "      embdList+=list(vec)\n",
        "    embdList = np.array(embdList).reshape(-1)\n",
        "    Allw2v[i, 0:embdList.shape[0]] = embdList\n",
        "  else:\n",
        "    s = s[:20]\n",
        "    embdList = []\n",
        "    for j in range(len(s)):\n",
        "      word = s[j]\n",
        "      vec = wv[word]\n",
        "      embdList+=list(vec)\n",
        "    embdList = np.array(embdList).reshape(-1)\n",
        "    Allw2v[i, 0:embdList.shape[0]] = embdList\n",
        "\n",
        "index = [num for num in range(Allw2v.shape[0])]\n",
        "columns = [num for num in range(Allw2v.shape[1])]\n",
        "Allw2vDF = pd.DataFrame(data=Allw2v,index = index, columns=columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1bUyAdwjSzp",
        "outputId": "762450d9-4f12-4ec8-bc59-f44285761ac0"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "labels = df['Label']\n",
        "w2vTrain, w2vTest, w2vTrainLabels, w2vTestLabels = train_test_split(Allw2vDF,labels)  #train-test split \n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB().fit(w2vTrain, w2vTrainLabels)\n",
        "nbPreds =nb.predict(w2vTest)\n",
        "nbAcc = np.mean(nbPreds==w2vTestLabels)\n",
        "print(\"NB done\")\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression().fit(w2vTrain, w2vTrainLabels)\n",
        "lrPreds = lr.predict(w2vTest)\n",
        "lrAcc = np.mean(lrPreds==w2vTestLabels)\n",
        "print(\"LR done\")\n",
        "#SVM\n",
        "svm = SVC().fit(w2vTrain, w2vTrainLabels)\n",
        "svmPreds = svm.predict(w2vTest)\n",
        "svmAcc = np.mean(svmPreds==w2vTestLabels)\n",
        "\n",
        "print(\"Word2Vec embedding results ..\\n\")\n",
        "print(\"Naive Bayes Acc: \" + str(nbAcc)+\"\\n\")\n",
        "print(\"LR Acc: \" + str(lrAcc)+\"\\n\")\n",
        "print(\"SVM Acc: \" + str(svmAcc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB done\n",
            "LR done\n",
            "Word2Vec embedding results ..\n",
            "\n",
            "Naive Bayes Acc: 0.5152838427947598\n",
            "\n",
            "LR Acc: 0.7671033478893741\n",
            "\n",
            "SVM Acc: 0.8034934497816594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeYhn4yTlh9-"
      },
      "source": [
        "Ad-hoc exploration of why pre-trained Word2Vec is doing poorly => print out the number of words we are throwing out on average in the first 20 words of a review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnycTVqql5XY",
        "outputId": "fe390d35-186d-4b16-b9ae-2e1500a318bb"
      },
      "source": [
        "ListOfList = []\n",
        "sents = list(df['Sentence'])\n",
        "for i in range(len(sents)):\n",
        "  s = sents[i]\n",
        "  ListOfList.append(s.split())\n",
        "\n",
        "\n",
        "avgs = []\n",
        "for i in range(len(ListOfList)):\n",
        "  if (i%100==0):\n",
        "    print(i)\n",
        "  s = ListOfList[i] #s = list of words in sentence\n",
        "  rem = []\n",
        "  count = 0\n",
        "  for l in range(len(s)):\n",
        "    word = s[l]\n",
        "    if word not in wv.index2word:\n",
        "      count+=1\n",
        "  avg = count/len(s)\n",
        "  avgs.append(avg)\n",
        "np.mean(avgs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03670074095199418"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPXM9K2qamn"
      },
      "source": [
        "Roughly 3.5% of the words in a sentence are missing and so are excluded. That is, barely any words in the review are exclcuded when we convert the sentence to a vector. This indicates that even the pre-trained word2vecv isnt all that much better than n-gram techniques. If the words that are excluded arenâ€™t really indicative of emotion or sentiment, then we can throw this method out. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGX4w2uvsY6P"
      },
      "source": [
        "Ad-Hoc Exploration of the Type of Words that are missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fJ6FauPsfua",
        "outputId": "d9a90314-36dd-4404-b922-46544b141d1a"
      },
      "source": [
        "excludedwords = []\n",
        "for i in range(len(ListOfList)):\n",
        "  if (i%100==0):\n",
        "    print(i)\n",
        "  s = ListOfList[i] #s = list of words in sentence\n",
        "  for l in range(len(s)):\n",
        "    word = s[l]\n",
        "    if word not in wv.index2word:\n",
        "      if word not in excludedwords:\n",
        "        excludedwords.append(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM_8dVLav20w",
        "outputId": "fc0fe849-8db7-40ca-e293-dda05ccb513f"
      },
      "source": [
        "excludedwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['factbased',\n",
              " 'sexobsessed',\n",
              " 'hollander',\n",
              " 'didnt',\n",
              " 'soundwise',\n",
              " 'krussel',\n",
              " 'wellwell',\n",
              " 'humour',\n",
              " 'doesnt',\n",
              " '15',\n",
              " 'jabra',\n",
              " 'eargels',\n",
              " '10',\n",
              " '2160',\n",
              " 'tracfone',\n",
              " '90',\n",
              " '20',\n",
              " 'mebunch',\n",
              " '785',\n",
              " 'trond',\n",
              " 'fausa',\n",
              " 'aurvÃ¥g',\n",
              " '90s',\n",
              " 'spinn',\n",
              " 'greenstreet',\n",
              " 'yardley',\n",
              " 'vinegrette',\n",
              " '30',\n",
              " 'gloveseverything',\n",
              " 'nothingi',\n",
              " 'v3i',\n",
              " 'glassesthe',\n",
              " 'overhip',\n",
              " 'underservices',\n",
              " '810',\n",
              " 'frequentyly',\n",
              " 'phones2',\n",
              " 'hitchcock',\n",
              " 'mishima',\n",
              " 'livingworking',\n",
              " 'schrader',\n",
              " '80s',\n",
              " 'woa',\n",
              " 'themeat',\n",
              " 'worldweariness',\n",
              " 'cinematographyif',\n",
              " 'thatsucked',\n",
              " 'stateoftheart',\n",
              " 'wasnt',\n",
              " 'actingwise',\n",
              " 'noircrimedrama',\n",
              " 'belmondo',\n",
              " 'melville',\n",
              " 'crocdodile',\n",
              " 'eccleston',\n",
              " 'kieslowski',\n",
              " 'favourite',\n",
              " 'colours',\n",
              " 'anythinga',\n",
              " 'roths',\n",
              " 'coppola',\n",
              " '25',\n",
              " 'nicolas',\n",
              " 'roeg',\n",
              " 'postinos',\n",
              " '1998',\n",
              " 'meeverything',\n",
              " '35',\n",
              " 'foodand',\n",
              " '18th',\n",
              " 'jutland',\n",
              " 'longwearing',\n",
              " '350',\n",
              " 'jabra350',\n",
              " 'pg13',\n",
              " '5020',\n",
              " 'comfortible',\n",
              " '24',\n",
              " 'contstruct',\n",
              " '510',\n",
              " 'comparablypriced',\n",
              " '70s',\n",
              " 'onid',\n",
              " 'classywarm',\n",
              " 'connerys',\n",
              " 'candace',\n",
              " 'bergens',\n",
              " 'hustons',\n",
              " 'kanalys',\n",
              " 'roosevelts',\n",
              " 'heavyit',\n",
              " 'herewhat',\n",
              " 'accessoryone',\n",
              " '40',\n",
              " 'securly',\n",
              " 'macbeth',\n",
              " 'connery',\n",
              " 'outhe',\n",
              " 'columbo',\n",
              " 'specialand',\n",
              " 'antena',\n",
              " 'pseudosatanic',\n",
              " 'filmsomething',\n",
              " '1928',\n",
              " 'bt50',\n",
              " '5of',\n",
              " 'savalas',\n",
              " '54',\n",
              " 'mansonites',\n",
              " 'ackerman',\n",
              " '815pm',\n",
              " 'cardellini',\n",
              " 'snider',\n",
              " 'thrillerhorror',\n",
              " 'fulci',\n",
              " 'leni',\n",
              " 'laselva',\n",
              " 'taelons',\n",
              " 'grey',\n",
              " 'cancelling',\n",
              " 'juano',\n",
              " 'gx2',\n",
              " 'ryans',\n",
              " 'lifemy',\n",
              " 'scifis',\n",
              " 'vandiver',\n",
              " 'buyit',\n",
              " 'booksomethats',\n",
              " 'burrittos',\n",
              " 'prosgood',\n",
              " 'sause',\n",
              " 'underacting',\n",
              " 'atmosphere1',\n",
              " 'idiotsavant',\n",
              " 'good4',\n",
              " 'gerardo',\n",
              " '1986',\n",
              " 'blueant',\n",
              " 'supertooth',\n",
              " 'onethis',\n",
              " 'juicehighy',\n",
              " 'hayao',\n",
              " 'miyazakis',\n",
              " 'ghibili',\n",
              " 'gake',\n",
              " 'ponyo',\n",
              " 'v115g',\n",
              " 'w810i',\n",
              " 'titta',\n",
              " 'girolamo',\n",
              " 'hasnt',\n",
              " '12',\n",
              " 'crowes',\n",
              " 'lestat',\n",
              " 'townsend',\n",
              " 'plantronics',\n",
              " 'methe',\n",
              " '1973',\n",
              " 'affleck',\n",
              " 'wifetobe',\n",
              " 'chanceit',\n",
              " '50',\n",
              " 'photographycinematography',\n",
              " 'keira',\n",
              " 'knightley',\n",
              " 'ca42',\n",
              " '2005',\n",
              " 's710a',\n",
              " 'samsungcrap',\n",
              " 'e715',\n",
              " 'bougth',\n",
              " 'l7c',\n",
              " 'cibo',\n",
              " 'unacceptableunless',\n",
              " 'greatno',\n",
              " 'bottowm',\n",
              " 'lineanother',\n",
              " 'ayce',\n",
              " 'selfdiscovery',\n",
              " 'palmtopcameracellphone',\n",
              " 'filmmostly',\n",
              " 'andddd',\n",
              " 'be3',\n",
              " 'gorman',\n",
              " 'bechard',\n",
              " 'characterage',\n",
              " 'autoanswer',\n",
              " 'updatewent',\n",
              " 'pissd',\n",
              " '400',\n",
              " 'brokeni',\n",
              " 'rebootsoverall',\n",
              " 'wellbalanced',\n",
              " 'watkins',\n",
              " 'disapoinment',\n",
              " 'crowdpleaserthis',\n",
              " 'mickeys',\n",
              " '80',\n",
              " 'smashburger',\n",
              " 'firstperson',\n",
              " 's11',\n",
              " 'babie',\n",
              " 'loudglad',\n",
              " 'koteasjack',\n",
              " 'palance',\n",
              " 'nakedbilly',\n",
              " 'drago',\n",
              " 'thorsen',\n",
              " 'mesmerising',\n",
              " 'cheesecurds',\n",
              " 'dualpurpose',\n",
              " '45',\n",
              " 'noca',\n",
              " 'burtons',\n",
              " 'labute',\n",
              " 'scrimm',\n",
              " 'violinplaying',\n",
              " 'havilland',\n",
              " 'duethe',\n",
              " 'lesserknown',\n",
              " '8530',\n",
              " 'itdefinitely',\n",
              " '23',\n",
              " 'cavier',\n",
              " '100',\n",
              " 'absolutel',\n",
              " 'bt250v',\n",
              " '20th',\n",
              " 'foxs',\n",
              " '1948',\n",
              " 'wrotedirected',\n",
              " '1995',\n",
              " 'wrongfirst',\n",
              " 'achille',\n",
              " 'philippa',\n",
              " 'giovanni',\n",
              " 'hayworth',\n",
              " 'breakfastlunch',\n",
              " 'beall',\n",
              " 'endall',\n",
              " 'brainsucking',\n",
              " 'windresistant',\n",
              " 'v265',\n",
              " 'earset',\n",
              " 'earbugs',\n",
              " 'kristoffersen',\n",
              " 'peachykeen',\n",
              " '1010',\n",
              " 'fellowes',\n",
              " 'beateous',\n",
              " 'rubberpetroleum',\n",
              " 'disneypixars',\n",
              " 'miyazaki',\n",
              " 'handdrawn',\n",
              " '95',\n",
              " 'theatres',\n",
              " '010',\n",
              " 'screenthis',\n",
              " 'topvery',\n",
              " 'buyerbe',\n",
              " 'reversestereotypes',\n",
              " 'yaall',\n",
              " 'theatre',\n",
              " 'satifying',\n",
              " 'bluetoothmotorola',\n",
              " 'hs850',\n",
              " '2007',\n",
              " 'gere',\n",
              " 'oyvey',\n",
              " 'howeverthe',\n",
              " 'riingtones',\n",
              " 'goremeister',\n",
              " 'lucio',\n",
              " 'gaudi',\n",
              " 'taylors',\n",
              " 'stanwycks',\n",
              " 'showthese',\n",
              " 'deuchebaggery',\n",
              " 'ravoli',\n",
              " 'chickenwith',\n",
              " 'cranberrymmmm',\n",
              " 'bitpim',\n",
              " 'internetto',\n",
              " 'phonethe',\n",
              " 'bluetoooth',\n",
              " 'sanyo',\n",
              " 'perpared',\n",
              " 'bluetooths',\n",
              " 'trumbull',\n",
              " 'motorolas',\n",
              " 'mayowell',\n",
              " 'lugosi',\n",
              " '110',\n",
              " 'flipphones',\n",
              " 'welldesigned',\n",
              " 'thoughtprovoking',\n",
              " 'isnt',\n",
              " 'sobaditsgood',\n",
              " 'sobaditsmemorable',\n",
              " 'muststop',\n",
              " 'irda',\n",
              " 'falwell',\n",
              " 'prettyoff',\n",
              " 'damian',\n",
              " 'hiro',\n",
              " '910',\n",
              " 'onedimensional',\n",
              " 'needshandsfree',\n",
              " 'bigbudget',\n",
              " '11',\n",
              " 'wellit',\n",
              " 'earpad',\n",
              " 'onlyi',\n",
              " 'errol',\n",
              " 'flynn',\n",
              " 'watsons',\n",
              " 'ericson',\n",
              " 'z500a',\n",
              " 'nonresearched',\n",
              " 'waaaaaayyyyyyyyyy',\n",
              " 'oconnor',\n",
              " '13',\n",
              " 'slowmotion',\n",
              " 'selfindulgent',\n",
              " 'tranquillity',\n",
              " 'selfsacrifice',\n",
              " 'stanwyck',\n",
              " 'minutesmajor',\n",
              " 'slowmoving',\n",
              " 'baxendale',\n",
              " 'cheerfull',\n",
              " 'amazingstylized',\n",
              " 'miniusb',\n",
              " 'tracfonewebsite',\n",
              " 'toactivate',\n",
              " 'puzzlesolving',\n",
              " 'pgrated',\n",
              " 'carrell',\n",
              " 'bluegreenscreen',\n",
              " 'meagre',\n",
              " 'headoverheels',\n",
              " 'heche',\n",
              " 'trainroller',\n",
              " 'noncliche',\n",
              " 'nevsky',\n",
              " 'quÃ©bec',\n",
              " '1971',\n",
              " 'baaaaaad',\n",
              " 'wellpaced',\n",
              " 'workingeating',\n",
              " 'h500',\n",
              " 'grtting',\n",
              " '744',\n",
              " 'v3c',\n",
              " 'servicecheck',\n",
              " 'wirefly',\n",
              " 'stari',\n",
              " 'cingularatt',\n",
              " 'movieit',\n",
              " 'bertolucci',\n",
              " 'problemvery',\n",
              " '2006',\n",
              " 'wordofmouth',\n",
              " 'etcits',\n",
              " 'beensteppedinandtrackedeverywhere',\n",
              " 'unacceptible',\n",
              " 'receptionsound',\n",
              " 'latifas',\n",
              " 'greatespecially',\n",
              " 'gooodd',\n",
              " 'hoursthe',\n",
              " 'thereplacement',\n",
              " 'blist',\n",
              " 'horrorsuspense',\n",
              " 'schr450',\n",
              " 'amazingrge',\n",
              " 'phonesmp3',\n",
              " 'romanticcharminghilariousand',\n",
              " 'adorablethe',\n",
              " 'funnyall',\n",
              " 'specialtoo',\n",
              " 'laughedkids',\n",
              " 'itbuy',\n",
              " 'outit',\n",
              " 'disapppointment',\n",
              " 'welldone',\n",
              " 'microsofts',\n",
              " 'ganoush',\n",
              " 'carlys',\n",
              " 'mclaglen',\n",
              " 'donlevy',\n",
              " 'mchattie',\n",
              " 'hendrikson',\n",
              " '5320',\n",
              " 'shakespears',\n",
              " 'loewenhielms',\n",
              " 'cailles',\n",
              " 'sarcophage',\n",
              " '70',\n",
              " 'costcos',\n",
              " 'malebonding',\n",
              " '750',\n",
              " 'thiswhen',\n",
              " 'hereas',\n",
              " 'managementoh',\n",
              " '42',\n",
              " 'zombiez',\n",
              " 'ramseys',\n",
              " 'mindbendingly',\n",
              " 'soyo',\n",
              " 'characterisation',\n",
              " 'itmy',\n",
              " 'phonebattery',\n",
              " 'seuss',\n",
              " 'upas',\n",
              " 'plantronincs',\n",
              " '1947',\n",
              " 'jenni',\n",
              " '15lb',\n",
              " '34ths',\n",
              " 'expertconnisseur',\n",
              " 'dennys',\n",
              " 'gentletouch',\n",
              " 'hunan',\n",
              " '17',\n",
              " 'crayonpencil',\n",
              " 'lifeoh',\n",
              " 'annes',\n",
              " 'shouldnt',\n",
              " 'tigerlilly',\n",
              " 'widmark',\n",
              " 'armand',\n",
              " 'assante',\n",
              " 'unrecommended',\n",
              " 'thomerson',\n",
              " 'shawarrrrrrma',\n",
              " 'estevezs',\n",
              " 'estevez',\n",
              " 'bellagio',\n",
              " 'steiners',\n",
              " 'worstannoying',\n",
              " 'realised',\n",
              " '40min',\n",
              " 'honeslty',\n",
              " 'jessice',\n",
              " 'actorsan',\n",
              " 'storytellinga',\n",
              " 'aailiyah',\n",
              " 'akasha',\n",
              " 'vx9900',\n",
              " 'youdo',\n",
              " 'bellucci',\n",
              " 'jaclyn',\n",
              " 'replacementr',\n",
              " 'threepack',\n",
              " 'austens',\n",
              " 'mandalay',\n",
              " 'ownerchef',\n",
              " 'owneryou',\n",
              " 'goldencrispy',\n",
              " 'noncustomer',\n",
              " 'poler',\n",
              " 'cutebut',\n",
              " 'goesthe',\n",
              " '70000',\n",
              " 'vegasthere',\n",
              " 'garfield',\n",
              " 'lilli',\n",
              " 'plmer',\n",
              " 'leeand',\n",
              " 'screenjames',\n",
              " 'verizons',\n",
              " '8125',\n",
              " '1979',\n",
              " 'rickman',\n",
              " 'stowe',\n",
              " 'phonemy',\n",
              " 'auju',\n",
              " '30s',\n",
              " 'mirrormask',\n",
              " 'boringpointless',\n",
              " 'selfrespecting',\n",
              " 'rpger',\n",
              " 'excalibur',\n",
              " 'incrediable',\n",
              " 'chargelife',\n",
              " 'charismafree',\n",
              " 'hummh',\n",
              " 'foxx',\n",
              " 'spacek',\n",
              " 'quaid',\n",
              " 'directtovideo',\n",
              " 'ironside',\n",
              " 'receptiona',\n",
              " 'veggitarian',\n",
              " 'crackedi',\n",
              " 'eyepleasing',\n",
              " 'v325i',\n",
              " 'veganveggie',\n",
              " '1980s',\n",
              " 'nobu',\n",
              " '8525',\n",
              " 'handsdown',\n",
              " 'uhura',\n",
              " 'excellentangel',\n",
              " 'adorablehis',\n",
              " 'scaredand',\n",
              " 'whenscamp',\n",
              " 'beforei',\n",
              " 'movieits',\n",
              " '20the',\n",
              " '375',\n",
              " 'yearsgreat',\n",
              " 'huston',\n",
              " 'joyces',\n",
              " 'replaceeasy',\n",
              " 'filmiing',\n",
              " 'abovepretty',\n",
              " 'itfriendly',\n",
              " '1949',\n",
              " 'mst3k',\n",
              " 'deffinitely',\n",
              " 'upandcoming',\n",
              " 'dependant',\n",
              " 'hoffmans',\n",
              " 'perabo',\n",
              " 'purcashed',\n",
              " 'trythe',\n",
              " '18',\n",
              " 'custer',\n",
              " '700w',\n",
              " 'baileys',\n",
              " 'eiko',\n",
              " 'ishioka',\n",
              " 'coppolas',\n",
              " 'shakespear',\n",
              " '1199',\n",
              " 'ohsomature',\n",
              " 'neighbourgirl',\n",
              " 'jx10',\n",
              " 'dysfunctionhe',\n",
              " 'problems\\x97the',\n",
              " 'rightthe',\n",
              " 'cakeohhh',\n",
              " 'chodorov',\n",
              " 'negulesco',\n",
              " 'belowpar',\n",
              " 'actingeven',\n",
              " 'rochonwas',\n",
              " 'frequently4',\n",
              " 'bussell',\n",
              " 'pepperand',\n",
              " 'toneoverall',\n",
              " 'paolo',\n",
              " 'sorrentino',\n",
              " 'ngage',\n",
              " 'tvnever',\n",
              " 'reoccurebottom',\n",
              " 'd807wrongly',\n",
              " 'd807',\n",
              " 'fliptop',\n",
              " '680',\n",
              " 'girlfriendboyfriend',\n",
              " 'zombiestudents',\n",
              " 'travled',\n",
              " 'wilkinsons',\n",
              " 'duris',\n",
              " 'situations1',\n",
              " 'shatner',\n",
              " 'nimoy',\n",
              " 'hitchcocks',\n",
              " 'lowbudget',\n",
              " 'buldogis',\n",
              " 'startac',\n",
              " 'musicincluding',\n",
              " 'ortolani',\n",
              " 'thoughtsgabriels',\n",
              " 'danceall',\n",
              " 'nc17',\n",
              " 'nonfancy',\n",
              " '325',\n",
              " 'badwellits',\n",
              " 'upway',\n",
              " 'cingulair',\n",
              " 'applifies',\n",
              " 'selfpreservation',\n",
              " 'good7',\n",
              " '2000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IzRmpaMqUUT"
      },
      "source": [
        "Seems like pretrained word2vec isnt a good emebdding technique for our purposes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jUrvbmqe9G"
      },
      "source": [
        "Let's try Doc2Vec embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZtSvAdqjJ7"
      },
      "source": [
        "import gensim, warnings\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "sentences = list(df['Sentence'])\n",
        "ListOfList = []\n",
        "for s in sentences:\n",
        "  ListOfList.append(s.split()) #list of sentences but each sentence is itself a list of the words\n",
        "\n",
        "TaggedDocs = [TaggedDocument(d, [i]) for i, d in enumerate(ListOfList)]\n",
        "\n",
        "model = Doc2Vec(TaggedDocs, vector_size = 500, window = 4, min_count = 1, epochs = 100)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HmnhkqQtRpT"
      },
      "source": [
        "model.docvecs[index] will return sentence embedding of the sentence tagged w/ index. model.wv[word] will return word embedding of the word: word "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Q-anTAsg0C",
        "outputId": "e3543df2-e0d8-43c0-c202-d8907791499f"
      },
      "source": [
        "doc2vecAr = np.zeros((df.shape[0], 500)) #500 since thats the vector_size param in doc2vec function call\n",
        "\n",
        "for i in range(doc2vecAr.shape[0]):\n",
        "  doc2vecAr[i,:] = model.docvecs[i]\n",
        "\n",
        "index = [num for num in range(doc2vecAr.shape[0])]\n",
        "columns = [num for num in range(500)]\n",
        "d2vDF = pd.DataFrame(data=doc2vecAr,index = index, columns=columns)\n",
        "\n",
        "d2vTrain, d2vTest, d2vTrainLabels, d2vTestLabels = train_test_split(d2vDF, df['Label'])\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB().fit(d2vTrain, d2vTrainLabels)\n",
        "nbPreds =nb.predict(d2vTest)\n",
        "nbAcc = np.mean(nbPreds==d2vTestLabels)\n",
        "print(\"NB done\")\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression().fit(d2vTrain, d2vTrainLabels)\n",
        "lrPreds = lr.predict(d2vTest)\n",
        "lrAcc = np.mean(lrPreds==d2vTestLabels)\n",
        "print(\"LR done\")\n",
        "#SVM\n",
        "svm = SVC().fit(d2vTrain, d2vTrainLabels)\n",
        "svmPreds = svm.predict(d2vTest)\n",
        "svmAcc = np.mean(svmPreds==d2vTestLabels)\n",
        "\n",
        "print(\"Doc2Vec embedding results ..\\n\")\n",
        "print(\"Naive Bayes Acc: \" + str(nbAcc)+\"\\n\")\n",
        "print(\"LR Acc: \" + str(lrAcc)+\"\\n\")\n",
        "print(\"SVM Acc: \" + str(svmAcc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB done\n",
            "LR done\n",
            "Doc2Vec embedding results ..\n",
            "\n",
            "Naive Bayes Acc: 0.5720524017467249\n",
            "\n",
            "LR Acc: 0.7161572052401747\n",
            "\n",
            "SVM Acc: 0.7001455604075691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzOEvyCPywwH"
      },
      "source": [
        "Poor performance when compared to n-gram type embeddings techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXhyEmrGy6gU"
      },
      "source": [
        "Let's combine word2vec and doc2vec. The d2v model stores representations for not only the sentence but also for each word. So, if we consider a fixed length of words for all sentences (let's say 15 this time). We can get, for EACH sentence, an embedding that has the following number of features: vector size + 15*vector size. If the sentence is less than 15 words, we pad the end with zeros. So, if we set vector_size to be 30, than we have 30 + 15*30 = 480 features. If we set it to 50, we get 50 + 15*50 = 800 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0iTmfECy6DO",
        "outputId": "260d8270-c3d1-41ec-b631-6658b28fd0f3"
      },
      "source": [
        "import builtins\n",
        "model = Doc2Vec(TaggedDocs, vector_size = 50, window = 4, min_count = 1, epochs = 100)  \n",
        "doc2vecAr = np.zeros((df.shape[0], 800))\n",
        "for i in range(df.shape[0]):\n",
        "  s = ListOfList[i] #s = sentence i has a list\n",
        "  docEmbd = list(model.docvecs[i]) #list of the sentence embed vector\n",
        "  wordEmbds = []\n",
        "  if (builtins.len(s) > 15):\n",
        "    s = s[:15]\n",
        "  for w in s:\n",
        "    wordEmbds+=list(model.wv[w])\n",
        "  length = builtins.len(s)\n",
        "  finalEmbd = np.array(docEmbd+wordEmbds)\n",
        "  doc2vecAr[i,0:(length*50+50)] = finalEmbd\n",
        "\n",
        "index = [num for num in range(doc2vecAr.shape[0])]\n",
        "columns = [num for num in range(doc2vecAr.shape[1])]\n",
        "d2vDF = pd.DataFrame(data=doc2vecAr,index = index, columns=columns)\n",
        "\n",
        "d2vTrain, d2vTest, d2vTrainLabels, d2vTestLabels = train_test_split(d2vDF, df['Label'])\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB().fit(d2vTrain, d2vTrainLabels)\n",
        "nbPreds =nb.predict(d2vTest)\n",
        "nbAcc = np.mean(nbPreds==d2vTestLabels)\n",
        "print(\"NB done\")\n",
        "#Logistic Regression\n",
        "lr = LogisticRegression().fit(d2vTrain, d2vTrainLabels)\n",
        "lrPreds = lr.predict(d2vTest)\n",
        "lrAcc = np.mean(lrPreds==d2vTestLabels)\n",
        "print(\"LR done\")\n",
        "#SVM\n",
        "svm = SVC().fit(d2vTrain, d2vTrainLabels)\n",
        "svmPreds = svm.predict(d2vTest)\n",
        "svmAcc = np.mean(svmPreds==d2vTestLabels)\n",
        "\n",
        "print(\"Doc2Vec embedding results ..\\n\")\n",
        "print(\"Naive Bayes Acc: \" + str(nbAcc)+\"\\n\")\n",
        "print(\"LR Acc: \" + str(lrAcc)+\"\\n\")\n",
        "print(\"SVM Acc: \" + str(svmAcc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB done\n",
            "LR done\n",
            "Doc2Vec embedding results ..\n",
            "\n",
            "Naive Bayes Acc: 0.6346433770014556\n",
            "\n",
            "LR Acc: 0.6812227074235808\n",
            "\n",
            "SVM Acc: 0.7030567685589519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXD-DzUdtwe_"
      },
      "source": [
        "Safe to conlcude that no \"flavor\" of doc2vec has been shown to be all that effective. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1GXkQpSPyk8"
      },
      "source": [
        "LSTM RNN (Logic and code based on https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvgYcLoO37ys"
      },
      "source": [
        "COULD NOT IMPLEMENT BY DEADLINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFvNp2UT1VHR"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZfAE7aCQPCM"
      },
      "source": [
        "from collections import Counter\n",
        "sents = list(df['Sentence'])\n",
        "ListOfList = []\n",
        "for s in sents:\n",
        "  sentList = s.split()\n",
        "  ListOfList.append(sentList)\n",
        "#ListOfList is a list of each sentence where each sentence is itself a list of the words in the sentence \n",
        "allwords = (' '.join(sents)).split() #vocabulary of the corpus \n",
        "counter = Counter(allwords)\n",
        "wordsCount = counter.most_common(len(allwords)) #most common words have lowest indices\n",
        "\n",
        "\n",
        "# creating a dict\n",
        "word_dict = {w:i+1 for i,(w,c) in enumerate(wordsCount)}\n",
        "\n",
        "#convert sentences to list of integers \n",
        "listOfIntList = []\n",
        "for s in ListOfList:\n",
        "  s2int = []\n",
        "  for w in s:\n",
        "    s2int.append(word_dict[w])\n",
        "  listOfIntList.append(s2int)\n",
        "labels = df['Label']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "OSITYUukGB0u",
        "outputId": "3c96c806-4257-4e6b-ed2d-664563231763"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "reviewLens = []\n",
        "for l in listOfIntList:\n",
        "  reviewLens.append(len(l))\n",
        "plt.hist(np.array(reviewLens), bins=20,range=(0,30)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 97., 345., 718., 307., 479., 183., 255.,  93., 120.,  50.,  38.,\n",
              "         13.,  14.,   6.,  10.,   2.,   4.,   1.,   2.,   2.]),\n",
              " array([ 0. ,  1.5,  3. ,  4.5,  6. ,  7.5,  9. , 10.5, 12. , 13.5, 15. ,\n",
              "        16.5, 18. , 19.5, 21. , 22.5, 24. , 25.5, 27. , 28.5, 30. ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARQElEQVR4nO3dbYwdV33H8e+veQAUKM7D1rJsU6fFAqGqJOkqDQIhSgQiSYVTCdJEbeNGkcyLUIGoVFzeAFUrmarlIVJl5BJapwJCGkhjQUqxTBDlRQIbCIHE0CxRItty7OUhgRABCvz7Yo/hxqy9d3fvevcevh/p6p45c+bOORrtb2fPzsxNVSFJ6stvrHQHJEmjZ7hLUocMd0nqkOEuSR0y3CWpQ6evdAcAzjvvvNq0adNKd0OSxsq99977naqamGvdqgj3TZs2MTU1tdLdkKSxkuTRE61zWkaSOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjq0Ku5QHVebtn960ds+suOKEfZEkp7JM3dJ6tC84Z7kRUnuG3j9IMlbk5yTZG+Sh9r72a19ktyYZDrJ/UkuWv5hSJIGzRvuVfWtqrqgqi4A/gB4Crgd2A7sq6rNwL62DHAZsLm9tgE7l6PjkqQTW+i0zKXAt6vqUWALsLvV7waubOUtwM01625gTZJ1I+mtJGkoCw33q4GPtfLaqjrcyo8Ba1t5PXBgYJuDre4ZkmxLMpVkamZmZoHdkCSdzNDhnuRM4PXAfx6/rqoKqIXsuKp2VdVkVU1OTMz5rHlJ0iIt5Mz9MuArVXWkLR85Nt3S3o+2+kPAxoHtNrQ6SdIpspBwv4ZfTskA7AG2tvJW4I6B+mvbVTOXAE8MTN9Ikk6BoW5iSnIW8BrgTQPVO4Bbk1wPPApc1ervBC4Hppm9sua6kfVWkjSUocK9qn4EnHtc3XeZvXrm+LYF3DCS3kmSFsU7VCWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1KGhwj3JmiS3Jflmkv1JXpbknCR7kzzU3s9ubZPkxiTTSe5PctHyDkGSdLxhz9w/AHymql4MvBTYD2wH9lXVZmBfWwa4DNjcXtuAnSPtsSRpXvOGe5LnA68EbgKoqp9W1ePAFmB3a7YbuLKVtwA316y7gTVJ1o2855KkExrmzP18YAb4tyRfTfKhJGcBa6vqcGvzGLC2ldcDBwa2P9jqniHJtiRTSaZmZmYWPwJJ0q8YJtxPBy4CdlbVhcCP+OUUDABVVUAtZMdVtauqJqtqcmJiYiGbSpLmMUy4HwQOVtU9bfk2ZsP+yLHplvZ+tK0/BGwc2H5Dq5MknSLzhntVPQYcSPKiVnUp8CCwB9ja6rYCd7TyHuDadtXMJcATA9M3kqRT4PQh2/0V8JEkZwIPA9cx+4vh1iTXA48CV7W2dwKXA9PAU62tJOkUGircq+o+YHKOVZfO0baAG5bYL0nSEniHqiR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktShocI9ySNJvp7kviRTre6cJHuTPNTez271SXJjkukk9ye5aDkHIEn6VQs5c/+jqrqgqo59UfZ2YF9VbQb2tWWAy4DN7bUN2DmqzkqShrOUaZktwO5W3g1cOVB/c826G1iTZN0S9iNJWqBhw72Azya5N8m2Vre2qg638mPA2lZeDxwY2PZgq5MknSKnD9nuFVV1KMlvAXuTfHNwZVVVklrIjtsviW0AL3jBCxayqSRpHkOduVfVofZ+FLgduBg4cmy6pb0fbc0PARsHNt/Q6o7/zF1VNVlVkxMTE4sfgSTpV8wb7knOSvK8Y2XgtcA3gD3A1tZsK3BHK+8Brm1XzVwCPDEwfSNJOgWGmZZZC9ye5Fj7j1bVZ5J8Gbg1yfXAo8BVrf2dwOXANPAUcN3Iey1JOql5w72qHgZeOkf9d4FL56gv4IaR9E6StCjeoSpJHRr2ahmtIpu2f3rR2z6y44oR9kTSauWZuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDg0d7klOS/LVJJ9qy+cnuSfJdJKPJzmz1T+rLU+39ZuWp+uSpBNZyJn7W4D9A8vvAd5XVS8Evg9c3+qvB77f6t/X2kmSTqGhwj3JBuAK4ENtOcCrgdtak93Ala28pS3T1l/a2kuSTpFhz9zfD/wN8PO2fC7weFU93ZYPAutbeT1wAKCtf6K1f4Yk25JMJZmamZlZZPclSXOZN9yT/DFwtKruHeWOq2pXVU1W1eTExMQoP1qSfu2dPkSblwOvT3I58GzgN4EPAGuSnN7OzjcAh1r7Q8BG4GCS04HnA98dec8lSSc075l7Vf1tVW2oqk3A1cDnqurPgLuAN7RmW4E7WnlPW6at/1xV1Uh7LUk6qaVc5/524G1JppmdU7+p1d8EnNvq3wZsX1oXJUkLNcy0zC9U1eeBz7fyw8DFc7T5MfDGEfRNkrRI3qEqSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUMLevxAjzZt//RKd0GSRs4zd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6tC84Z7k2Um+lORrSR5I8u5Wf36Se5JMJ/l4kjNb/bPa8nRbv2l5hyBJOt4wjx/4CfDqqnoyyRnAF5P8N/A24H1VdUuSDwLXAzvb+/er6oVJrgbeA/zpMvV/bPnYA0nLad4z95r1ZFs8o70KeDVwW6vfDVzZylvaMm39pUkysh5LkuY11Jx7ktOS3AccBfYC3wYer6qnW5ODwPpWXg8cAGjrnwDOneMztyWZSjI1MzOztFFIkp5hqHCvqp9V1QXABuBi4MVL3XFV7aqqyaqanJiYWOrHSZIGLOhqmap6HLgLeBmwJsmxOfsNwKFWPgRsBGjrnw98dyS9lSQNZZirZSaSrGnl5wCvAfYzG/JvaM22Ane08p62TFv/uaqqUXZaknRyw1wtsw7YneQ0Zn8Z3FpVn0ryIHBLkr8Hvgrc1NrfBPxHkmnge8DVy9BvSdJJzBvuVXU/cOEc9Q8zO/9+fP2PgTeOpHeSpEXxDlVJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjo0zE1M0i8s5VHFj+y4YoQ9kXQynrlLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUPzhnuSjUnuSvJgkgeSvKXVn5Nkb5KH2vvZrT5JbkwyneT+JBct9yAkSc80zJn708BfV9VLgEuAG5K8BNgO7KuqzcC+tgxwGbC5vbYBO0fea0nSSc0b7lV1uKq+0so/BPYD64EtwO7WbDdwZStvAW6uWXcDa5KsG3nPJUkntKA59ySbgAuBe4C1VXW4rXoMWNvK64EDA5sdbHXHf9a2JFNJpmZmZhbYbUnSyQwd7kmeC3wCeGtV/WBwXVUVUAvZcVXtqqrJqpqcmJhYyKaSpHkMFe5JzmA22D9SVZ9s1UeOTbe096Ot/hCwcWDzDa1OknSKDHO1TICbgP1V9d6BVXuAra28FbhjoP7adtXMJcATA9M3kqRTYJjvUH058BfA15Pc1+reAewAbk1yPfAocFVbdydwOTANPAVcN9IeS5LmNW+4V9UXgZxg9aVztC/ghiX2S8tkKV9wLWl8eIeqJHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdWiYr9mTVtxSvkHqkR1XjLAn0njwzF2SOmS4S1KH5g33JB9OcjTJNwbqzkmyN8lD7f3sVp8kNyaZTnJ/kouWs/OSpLkNc+b+78DrjqvbDuyrqs3AvrYMcBmwub22ATtH001J0kLMG+5V9QXge8dVbwF2t/Ju4MqB+ptr1t3AmiTrRtVZSdJwFjvnvraqDrfyY8DaVl4PHBhod7DV/Yok25JMJZmamZlZZDckSXNZ8qWQVVVJahHb7QJ2AUxOTi54+2OWcomcJPVqseF+JMm6qjrcpl2OtvpDwMaBdhtaneQvYukUWuy0zB5gaytvBe4YqL+2XTVzCfDEwPSNJOkUmffMPcnHgFcB5yU5CLwT2AHcmuR64FHgqtb8TuByYBp4CrhuGfosSZrHvOFeVdecYNWlc7Qt4IaldkqStDTeoSpJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkN+hqu75/av6deSZuyR1yHCXpA4Z7pLUIefcpZNY6jPonbPXSvHMXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXISyGlZbTUSykXy0swtSzhnuR1wAeA04APVdWO5diPpNHz2v4+jDzck5wG/AvwGuAg8OUke6rqwVHvS9LcVuovBq0ey3HmfjEwXVUPAyS5BdgCGO7Sr4FxnIpayV+Gy/WXznKE+3rgwMDyQeAPj2+UZBuwrS0+meRbi9zfecB3FrntauNYVp9exgGdjyXvWaGeLFHes6Tj8tsnWrFi/1Ctql3ArqV+TpKpqpocQZdWnGNZfXoZBziW1Wq5xrIcl0IeAjYOLG9odZKkU2Q5wv3LwOYk5yc5E7ga2LMM+5EkncDIp2Wq6ukkbwb+h9lLIT9cVQ+Mej8Dljy1s4o4ltWnl3GAY1mtlmUsqarl+FxJ0gry8QOS1CHDXZI6NNbhnuR1Sb6VZDrJ9pXuz1IkeSTJ15Pcl2RqpfuzEEk+nORokm8M1J2TZG+Sh9r72SvZx2GcYBzvSnKoHZf7kly+kn0cVpKNSe5K8mCSB5K8pdWP1XE5yTjG7rgkeXaSLyX5WhvLu1v9+UnuaTn28XYhytL3N65z7u0xB//HwGMOgGvG9TEHSR4BJqtq7G4ySfJK4Eng5qr6vVb3j8D3qmpH+8V7dlW9fSX7OZ8TjONdwJNV9U8r2beFSrIOWFdVX0nyPOBe4ErgLxmj43KScVzFmB2XJAHOqqonk5wBfBF4C/A24JNVdUuSDwJfq6qdS93fOJ+5/+IxB1X1U+DYYw50ilXVF4DvHVe9BdjdyruZ/YFc1U4wjrFUVYer6iut/ENgP7N3j4/VcTnJOMZOzXqyLZ7RXgW8Grit1Y/smIxzuM/1mIOxPOhNAZ9Ncm97NMO4W1tVh1v5MWDtSnZmid6c5P42bbOqpzHmkmQTcCFwD2N8XI4bB4zhcUlyWpL7gKPAXuDbwONV9XRrMrIcG+dw780rquoi4DLghjZF0IWanfsbz/k/2An8LnABcBj455XtzsIkeS7wCeCtVfWDwXXjdFzmGMdYHpeq+llVXcDsnfsXAy9ern2Nc7h39ZiDqjrU3o8CtzN74MfZkTZfemze9OgK92dRqupI+4H8OfCvjNFxafO6nwA+UlWfbNVjd1zmGsc4HxeAqnocuAt4GbAmybEbSkeWY+Mc7t085iDJWe2fRSQ5C3gt8I2Tb7Xq7QG2tvJW4I4V7MuiHQvC5k8Yk+PS/nl3E7C/qt47sGqsjsuJxjGOxyXJRJI1rfwcZi8G2c9syL+hNRvZMRnbq2UA2uVP7+eXjzn4hxXu0qIk+R1mz9Zh9pEQHx2nsST5GPAqZh/DegR4J/BfwK3AC4BHgauqalX/s/IE43gVs3/6F/AI8KaBOetVK8krgP8Fvg78vFW/g9n56rE5LicZxzWM2XFJ8vvM/sP0NGZPrG+tqr9rP/+3AOcAXwX+vKp+suT9jXO4S5LmNs7TMpKkEzDcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUof+HzcVCjeEGiPaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWTAPq8LGqnX"
      },
      "source": [
        "Let the max length of our reviews be 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46_VtENXRQAl",
        "outputId": "e626a300-7922-4929-a58d-f64d9da6c2cd"
      },
      "source": [
        "vecAr = np.zeros((len(listOfIntList), 7), dtype = int)\n",
        "for i, review in enumerate(listOfIntList):\n",
        "  review_len = len(review)  \n",
        "  if review_len <= 7:\n",
        "    zeros = list(np.zeros(7-review_len))\n",
        "    new = review+zeros\n",
        "  elif review_len > 7:\n",
        "    new = review[0:7]\n",
        "  vecAr[i,:] = np.array(new)\n",
        "vecAr[:10,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1338,  746,   78,    0,    0,    0,    0],\n",
              "       [2094,    9,   72, 1339,   65,  407,  221],\n",
              "       [2095,  951,  465,   91,    0,    0,    0],\n",
              "       [   4,   51,  623,   27,   38,  533,  200],\n",
              "       [ 270,  952,    0,    0,    0,    0,    0],\n",
              "       [ 953,  408,   92,    0,    0,    0,    0],\n",
              "       [ 251,  747,   93,  176,   53,  100,  466],\n",
              "       [  72,   94,  954,    0,    0,    0,    0],\n",
              "       [   1, 1340,    0,    0,    0,    0,    0],\n",
              "       [ 201,  748, 1341,  749,  109,    1, 1342]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA5vXUVJH0g9"
      },
      "source": [
        "vecAr is a numpy array of shape(num reviews x 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsZirpJSjTd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "labels = np.array(labels)\n",
        "trainX, testX, trainY, testY = train_test_split(vecAr,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmd-h8VTkaa"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_data = TensorDataset(torch.from_numpy(trainX), torch.from_numpy(trainY))\n",
        "test_data = TensorDataset(torch.from_numpy(testX), torch.from_numpy(testY))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 30\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMMZ9UKPT5oy"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #first param is dictionary size, second param is size of embedding \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, 1) #input is hidden_dim and output is of dim 1 \n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        # Note x is of shape (batch size, sentence length)\n",
        "        batch_size = x.size(0) \n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        embeds = torch.reshape(embeds, (7, batch_size, embedding_dim))\n",
        "        lstm_out, hidden = self.lstm(embeds) #input param hidden: 1 x batch_size x hidden_dim\n",
        "\n",
        "        #lstm shape => (7, batch_size, hidden_dim)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "\n",
        "        #fully conn layer\n",
        "        out = self.fc(lstm_out) #out = 7 * batch_size x 1 \n",
        "\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out) # sig_out = 7 * batch_sze x 1\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        #sig_out is of shape (batch_size x 1) \n",
        "        #hidden is of shape (1 x batch_size x hidden_dim)\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (is_cuda):\n",
        "            hidden = (weight.new(1, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(1, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(1, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(1, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "vocab_size = len(word_dict) + 1 #extra 1 for padding\n",
        "embedding_dim = 50\n",
        "hidden_dim = 256\n",
        "\n",
        "\n",
        "model = SentimentLSTM(vocab_size,embedding_dim, hidden_dim)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opck4SYpVdom",
        "outputId": "585eed1b-a629-472b-a817-be86da7a4295"
      },
      "source": [
        "clip = 5\n",
        "epochs = 10 \n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state \n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)   \n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs)\n",
        "        \n",
        "        # calculate the loss and perform backprop\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy/batch_size\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        print(train_acc)\n",
        "    \n",
        "    \"\"\"    \n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            output, h = model(inputs)\n",
        "            val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            \n",
        "            accuracy = acc(output,labels)\n",
        "            val_acc += accuracy\n",
        "            \n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}') \n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), '../working/state_dict.pt')\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')\"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(26, device='cuda:0')\n",
            "0.8666666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "1.8333333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "2.8000000000000003\n",
            "tensor(28, device='cuda:0')\n",
            "3.7333333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "4.7\n",
            "tensor(29, device='cuda:0')\n",
            "5.666666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "6.633333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "7.6000000000000005\n",
            "tensor(26, device='cuda:0')\n",
            "8.466666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "9.366666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "10.3\n",
            "tensor(26, device='cuda:0')\n",
            "11.166666666666668\n",
            "tensor(28, device='cuda:0')\n",
            "12.100000000000001\n",
            "tensor(26, device='cuda:0')\n",
            "12.966666666666669\n",
            "tensor(27, device='cuda:0')\n",
            "13.866666666666669\n",
            "tensor(26, device='cuda:0')\n",
            "14.733333333333336\n",
            "tensor(28, device='cuda:0')\n",
            "15.66666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "16.56666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "17.46666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "18.400000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "19.333333333333336\n",
            "tensor(28, device='cuda:0')\n",
            "20.26666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "21.233333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "22.2\n",
            "tensor(29, device='cuda:0')\n",
            "23.166666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "24.099999999999998\n",
            "tensor(30, device='cuda:0')\n",
            "25.099999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "26.066666666666663\n",
            "tensor(28, device='cuda:0')\n",
            "26.999999999999996\n",
            "tensor(29, device='cuda:0')\n",
            "27.96666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "28.899999999999995\n",
            "tensor(28, device='cuda:0')\n",
            "29.83333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "30.733333333333327\n",
            "tensor(28, device='cuda:0')\n",
            "31.66666666666666\n",
            "tensor(26, device='cuda:0')\n",
            "32.533333333333324\n",
            "tensor(30, device='cuda:0')\n",
            "33.533333333333324\n",
            "tensor(27, device='cuda:0')\n",
            "34.43333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "35.36666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "36.26666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "37.19999999999998\n",
            "tensor(26, device='cuda:0')\n",
            "38.06666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "38.96666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "39.866666666666646\n",
            "tensor(27, device='cuda:0')\n",
            "40.766666666666644\n",
            "tensor(26, device='cuda:0')\n",
            "41.63333333333331\n",
            "tensor(29, device='cuda:0')\n",
            "42.59999999999998\n",
            "tensor(26, device='cuda:0')\n",
            "43.46666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "44.39999999999998\n",
            "tensor(25, device='cuda:0')\n",
            "45.23333333333331\n",
            "tensor(29, device='cuda:0')\n",
            "46.19999999999998\n",
            "tensor(27, device='cuda:0')\n",
            "47.09999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "48.06666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "48.96666666666665\n",
            "tensor(26, device='cuda:0')\n",
            "49.833333333333314\n",
            "tensor(29, device='cuda:0')\n",
            "50.79999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "51.73333333333331\n",
            "tensor(25, device='cuda:0')\n",
            "52.56666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "53.46666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "54.366666666666646\n",
            "tensor(30, device='cuda:0')\n",
            "55.366666666666646\n",
            "tensor(27, device='cuda:0')\n",
            "56.266666666666644\n",
            "tensor(28, device='cuda:0')\n",
            "57.199999999999974\n",
            "tensor(28, device='cuda:0')\n",
            "58.133333333333304\n",
            "tensor(26, device='cuda:0')\n",
            "58.99999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "59.96666666666664\n",
            "tensor(27, device='cuda:0')\n",
            "60.86666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "61.83333333333331\n",
            "tensor(26, device='cuda:0')\n",
            "62.699999999999974\n",
            "tensor(28, device='cuda:0')\n",
            "0.9333333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "1.9\n",
            "tensor(28, device='cuda:0')\n",
            "2.833333333333333\n",
            "tensor(24, device='cuda:0')\n",
            "3.633333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "4.533333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "5.466666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "6.366666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "7.333333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "8.266666666666667\n",
            "tensor(26, device='cuda:0')\n",
            "9.133333333333335\n",
            "tensor(27, device='cuda:0')\n",
            "10.033333333333335\n",
            "tensor(25, device='cuda:0')\n",
            "10.866666666666669\n",
            "tensor(27, device='cuda:0')\n",
            "11.76666666666667\n",
            "tensor(25, device='cuda:0')\n",
            "12.600000000000003\n",
            "tensor(30, device='cuda:0')\n",
            "13.600000000000003\n",
            "tensor(30, device='cuda:0')\n",
            "14.600000000000003\n",
            "tensor(26, device='cuda:0')\n",
            "15.46666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "16.433333333333337\n",
            "tensor(28, device='cuda:0')\n",
            "17.36666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "18.300000000000004\n",
            "tensor(25, device='cuda:0')\n",
            "19.133333333333336\n",
            "tensor(28, device='cuda:0')\n",
            "20.06666666666667\n",
            "tensor(25, device='cuda:0')\n",
            "20.900000000000002\n",
            "tensor(27, device='cuda:0')\n",
            "21.8\n",
            "tensor(29, device='cuda:0')\n",
            "22.766666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "23.73333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "24.666666666666664\n",
            "tensor(25, device='cuda:0')\n",
            "25.499999999999996\n",
            "tensor(27, device='cuda:0')\n",
            "26.399999999999995\n",
            "tensor(29, device='cuda:0')\n",
            "27.36666666666666\n",
            "tensor(26, device='cuda:0')\n",
            "28.233333333333327\n",
            "tensor(28, device='cuda:0')\n",
            "29.16666666666666\n",
            "tensor(26, device='cuda:0')\n",
            "30.033333333333328\n",
            "tensor(28, device='cuda:0')\n",
            "30.96666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "31.933333333333326\n",
            "tensor(28, device='cuda:0')\n",
            "32.86666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "33.83333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "34.73333333333333\n",
            "tensor(26, device='cuda:0')\n",
            "35.599999999999994\n",
            "tensor(27, device='cuda:0')\n",
            "36.49999999999999\n",
            "tensor(30, device='cuda:0')\n",
            "37.49999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "38.46666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "39.46666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "40.36666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "41.36666666666666\n",
            "tensor(23, device='cuda:0')\n",
            "42.133333333333326\n",
            "tensor(26, device='cuda:0')\n",
            "42.99999999999999\n",
            "tensor(26, device='cuda:0')\n",
            "43.86666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "44.79999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "45.73333333333332\n",
            "tensor(30, device='cuda:0')\n",
            "46.73333333333332\n",
            "tensor(26, device='cuda:0')\n",
            "47.59999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "48.566666666666656\n",
            "tensor(29, device='cuda:0')\n",
            "49.533333333333324\n",
            "tensor(27, device='cuda:0')\n",
            "50.43333333333332\n",
            "tensor(26, device='cuda:0')\n",
            "51.29999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "52.19999999999999\n",
            "tensor(30, device='cuda:0')\n",
            "53.19999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "54.09999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "55.066666666666656\n",
            "tensor(25, device='cuda:0')\n",
            "55.89999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "56.83333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "57.76666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "58.69999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "59.63333333333331\n",
            "tensor(26, device='cuda:0')\n",
            "60.49999999999998\n",
            "tensor(30, device='cuda:0')\n",
            "61.49999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "62.46666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "0.9666666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "1.8666666666666667\n",
            "tensor(30, device='cuda:0')\n",
            "2.8666666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "3.7666666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "4.733333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "5.7\n",
            "tensor(26, device='cuda:0')\n",
            "6.566666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "7.5\n",
            "tensor(25, device='cuda:0')\n",
            "8.333333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "9.3\n",
            "tensor(28, device='cuda:0')\n",
            "10.233333333333334\n",
            "tensor(27, device='cuda:0')\n",
            "11.133333333333335\n",
            "tensor(26, device='cuda:0')\n",
            "12.000000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "12.933333333333335\n",
            "tensor(28, device='cuda:0')\n",
            "13.866666666666669\n",
            "tensor(28, device='cuda:0')\n",
            "14.800000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "15.733333333333336\n",
            "tensor(25, device='cuda:0')\n",
            "16.56666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "17.500000000000004\n",
            "tensor(27, device='cuda:0')\n",
            "18.400000000000002\n",
            "tensor(29, device='cuda:0')\n",
            "19.366666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "20.3\n",
            "tensor(28, device='cuda:0')\n",
            "21.233333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "22.2\n",
            "tensor(25, device='cuda:0')\n",
            "23.03333333333333\n",
            "tensor(26, device='cuda:0')\n",
            "23.9\n",
            "tensor(29, device='cuda:0')\n",
            "24.866666666666664\n",
            "tensor(26, device='cuda:0')\n",
            "25.73333333333333\n",
            "tensor(26, device='cuda:0')\n",
            "26.599999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "27.566666666666663\n",
            "tensor(27, device='cuda:0')\n",
            "28.46666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "29.433333333333326\n",
            "tensor(26, device='cuda:0')\n",
            "30.299999999999994\n",
            "tensor(28, device='cuda:0')\n",
            "31.233333333333327\n",
            "tensor(29, device='cuda:0')\n",
            "32.199999999999996\n",
            "tensor(27, device='cuda:0')\n",
            "33.099999999999994\n",
            "tensor(28, device='cuda:0')\n",
            "34.033333333333324\n",
            "tensor(27, device='cuda:0')\n",
            "34.93333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "35.86666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "36.76666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "37.69999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "38.63333333333331\n",
            "tensor(29, device='cuda:0')\n",
            "39.59999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "40.53333333333331\n",
            "tensor(29, device='cuda:0')\n",
            "41.49999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "42.46666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "43.39999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "44.366666666666646\n",
            "tensor(28, device='cuda:0')\n",
            "45.299999999999976\n",
            "tensor(29, device='cuda:0')\n",
            "46.266666666666644\n",
            "tensor(24, device='cuda:0')\n",
            "47.06666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "47.99999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "48.96666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "49.93333333333331\n",
            "tensor(29, device='cuda:0')\n",
            "50.89999999999998\n",
            "tensor(25, device='cuda:0')\n",
            "51.73333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "52.66666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "53.59999999999997\n",
            "tensor(25, device='cuda:0')\n",
            "54.43333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "55.36666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "56.33333333333331\n",
            "tensor(26, device='cuda:0')\n",
            "57.199999999999974\n",
            "tensor(29, device='cuda:0')\n",
            "58.16666666666664\n",
            "tensor(27, device='cuda:0')\n",
            "59.06666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "59.99999999999997\n",
            "tensor(26, device='cuda:0')\n",
            "60.86666666666664\n",
            "tensor(27, device='cuda:0')\n",
            "61.76666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "62.69999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "0.9\n",
            "tensor(27, device='cuda:0')\n",
            "1.8\n",
            "tensor(30, device='cuda:0')\n",
            "2.8\n",
            "tensor(29, device='cuda:0')\n",
            "3.7666666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "4.7\n",
            "tensor(30, device='cuda:0')\n",
            "5.7\n",
            "tensor(28, device='cuda:0')\n",
            "6.633333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "7.6000000000000005\n",
            "tensor(30, device='cuda:0')\n",
            "8.600000000000001\n",
            "tensor(29, device='cuda:0')\n",
            "9.566666666666668\n",
            "tensor(27, device='cuda:0')\n",
            "10.466666666666669\n",
            "tensor(27, device='cuda:0')\n",
            "11.366666666666669\n",
            "tensor(25, device='cuda:0')\n",
            "12.200000000000003\n",
            "tensor(29, device='cuda:0')\n",
            "13.16666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "14.133333333333336\n",
            "tensor(27, device='cuda:0')\n",
            "15.033333333333337\n",
            "tensor(30, device='cuda:0')\n",
            "16.03333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "17.000000000000004\n",
            "tensor(29, device='cuda:0')\n",
            "17.96666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "18.933333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "19.866666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "20.833333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "21.766666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "22.7\n",
            "tensor(27, device='cuda:0')\n",
            "23.599999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "24.566666666666663\n",
            "tensor(29, device='cuda:0')\n",
            "25.533333333333328\n",
            "tensor(28, device='cuda:0')\n",
            "26.46666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "27.433333333333326\n",
            "tensor(29, device='cuda:0')\n",
            "28.39999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "29.333333333333325\n",
            "tensor(29, device='cuda:0')\n",
            "30.29999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "31.266666666666655\n",
            "tensor(28, device='cuda:0')\n",
            "32.19999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "33.13333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "34.09999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "34.999999999999986\n",
            "tensor(27, device='cuda:0')\n",
            "35.899999999999984\n",
            "tensor(27, device='cuda:0')\n",
            "36.79999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "37.76666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "38.73333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "39.66666666666665\n",
            "tensor(25, device='cuda:0')\n",
            "40.499999999999986\n",
            "tensor(28, device='cuda:0')\n",
            "41.433333333333316\n",
            "tensor(30, device='cuda:0')\n",
            "42.433333333333316\n",
            "tensor(25, device='cuda:0')\n",
            "43.26666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "44.23333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "45.16666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "46.13333333333332\n",
            "tensor(25, device='cuda:0')\n",
            "46.966666666666654\n",
            "tensor(27, device='cuda:0')\n",
            "47.86666666666665\n",
            "tensor(26, device='cuda:0')\n",
            "48.73333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "49.69999999999999\n",
            "tensor(25, device='cuda:0')\n",
            "50.533333333333324\n",
            "tensor(29, device='cuda:0')\n",
            "51.49999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "52.46666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "53.36666666666666\n",
            "tensor(26, device='cuda:0')\n",
            "54.23333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "55.133333333333326\n",
            "tensor(27, device='cuda:0')\n",
            "56.033333333333324\n",
            "tensor(28, device='cuda:0')\n",
            "56.966666666666654\n",
            "tensor(27, device='cuda:0')\n",
            "57.86666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "58.79999999999998\n",
            "tensor(26, device='cuda:0')\n",
            "59.66666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "60.56666666666665\n",
            "tensor(26, device='cuda:0')\n",
            "61.433333333333316\n",
            "tensor(29, device='cuda:0')\n",
            "62.399999999999984\n",
            "tensor(29, device='cuda:0')\n",
            "63.36666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "0.9333333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "1.9333333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "2.8333333333333335\n",
            "tensor(28, device='cuda:0')\n",
            "3.7666666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "4.733333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "5.633333333333334\n",
            "tensor(26, device='cuda:0')\n",
            "6.5\n",
            "tensor(29, device='cuda:0')\n",
            "7.466666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "8.4\n",
            "tensor(26, device='cuda:0')\n",
            "9.266666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "10.200000000000001\n",
            "tensor(26, device='cuda:0')\n",
            "11.066666666666668\n",
            "tensor(30, device='cuda:0')\n",
            "12.066666666666668\n",
            "tensor(27, device='cuda:0')\n",
            "12.966666666666669\n",
            "tensor(29, device='cuda:0')\n",
            "13.933333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "14.900000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "15.833333333333336\n",
            "tensor(29, device='cuda:0')\n",
            "16.8\n",
            "tensor(29, device='cuda:0')\n",
            "17.766666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "18.766666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "19.73333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "20.63333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "21.566666666666663\n",
            "tensor(28, device='cuda:0')\n",
            "22.499999999999996\n",
            "tensor(29, device='cuda:0')\n",
            "23.46666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "24.399999999999995\n",
            "tensor(26, device='cuda:0')\n",
            "25.266666666666662\n",
            "tensor(25, device='cuda:0')\n",
            "26.099999999999994\n",
            "tensor(28, device='cuda:0')\n",
            "27.033333333333328\n",
            "tensor(27, device='cuda:0')\n",
            "27.933333333333326\n",
            "tensor(28, device='cuda:0')\n",
            "28.86666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "29.833333333333325\n",
            "tensor(29, device='cuda:0')\n",
            "30.79999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "31.766666666666655\n",
            "tensor(27, device='cuda:0')\n",
            "32.66666666666666\n",
            "tensor(26, device='cuda:0')\n",
            "33.533333333333324\n",
            "tensor(25, device='cuda:0')\n",
            "34.36666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "35.33333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "36.33333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "37.3\n",
            "tensor(27, device='cuda:0')\n",
            "38.199999999999996\n",
            "tensor(27, device='cuda:0')\n",
            "39.099999999999994\n",
            "tensor(27, device='cuda:0')\n",
            "39.99999999999999\n",
            "tensor(30, device='cuda:0')\n",
            "40.99999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "41.93333333333332\n",
            "tensor(27, device='cuda:0')\n",
            "42.83333333333332\n",
            "tensor(26, device='cuda:0')\n",
            "43.69999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "44.59999999999999\n",
            "tensor(30, device='cuda:0')\n",
            "45.59999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "46.499999999999986\n",
            "tensor(29, device='cuda:0')\n",
            "47.466666666666654\n",
            "tensor(27, device='cuda:0')\n",
            "48.36666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "49.26666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "50.23333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "51.16666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "52.06666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "52.96666666666665\n",
            "tensor(30, device='cuda:0')\n",
            "53.96666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "54.89999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "55.866666666666646\n",
            "tensor(29, device='cuda:0')\n",
            "56.833333333333314\n",
            "tensor(26, device='cuda:0')\n",
            "57.69999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "58.66666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "59.59999999999998\n",
            "tensor(30, device='cuda:0')\n",
            "60.59999999999998\n",
            "tensor(27, device='cuda:0')\n",
            "61.49999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "62.43333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "63.36666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "0.9666666666666667\n",
            "tensor(30, device='cuda:0')\n",
            "1.9666666666666668\n",
            "tensor(29, device='cuda:0')\n",
            "2.9333333333333336\n",
            "tensor(27, device='cuda:0')\n",
            "3.8333333333333335\n",
            "tensor(27, device='cuda:0')\n",
            "4.733333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "5.633333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "6.6000000000000005\n",
            "tensor(27, device='cuda:0')\n",
            "7.500000000000001\n",
            "tensor(28, device='cuda:0')\n",
            "8.433333333333334\n",
            "tensor(27, device='cuda:0')\n",
            "9.333333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "10.266666666666667\n",
            "tensor(30, device='cuda:0')\n",
            "11.266666666666667\n",
            "tensor(26, device='cuda:0')\n",
            "12.133333333333335\n",
            "tensor(30, device='cuda:0')\n",
            "13.133333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "14.100000000000001\n",
            "tensor(27, device='cuda:0')\n",
            "15.000000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "15.933333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "16.900000000000002\n",
            "tensor(27, device='cuda:0')\n",
            "17.8\n",
            "tensor(26, device='cuda:0')\n",
            "18.666666666666668\n",
            "tensor(27, device='cuda:0')\n",
            "19.566666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "20.53333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "21.499999999999996\n",
            "tensor(28, device='cuda:0')\n",
            "22.43333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "23.366666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "24.33333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "25.299999999999994\n",
            "tensor(29, device='cuda:0')\n",
            "26.26666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "27.233333333333324\n",
            "tensor(29, device='cuda:0')\n",
            "28.19999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "29.166666666666654\n",
            "tensor(26, device='cuda:0')\n",
            "30.03333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "30.999999999999986\n",
            "tensor(29, device='cuda:0')\n",
            "31.96666666666665\n",
            "tensor(30, device='cuda:0')\n",
            "32.966666666666654\n",
            "tensor(29, device='cuda:0')\n",
            "33.93333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "34.86666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "35.79999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "36.73333333333331\n",
            "tensor(26, device='cuda:0')\n",
            "37.59999999999998\n",
            "tensor(26, device='cuda:0')\n",
            "38.46666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "39.366666666666646\n",
            "tensor(28, device='cuda:0')\n",
            "40.299999999999976\n",
            "tensor(28, device='cuda:0')\n",
            "41.233333333333306\n",
            "tensor(29, device='cuda:0')\n",
            "42.199999999999974\n",
            "tensor(30, device='cuda:0')\n",
            "43.199999999999974\n",
            "tensor(30, device='cuda:0')\n",
            "44.199999999999974\n",
            "tensor(28, device='cuda:0')\n",
            "45.133333333333304\n",
            "tensor(29, device='cuda:0')\n",
            "46.09999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "47.0333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "47.99999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "48.89999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "49.79999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "50.76666666666664\n",
            "tensor(30, device='cuda:0')\n",
            "51.76666666666664\n",
            "tensor(26, device='cuda:0')\n",
            "52.633333333333304\n",
            "tensor(28, device='cuda:0')\n",
            "53.566666666666634\n",
            "tensor(28, device='cuda:0')\n",
            "54.499999999999964\n",
            "tensor(28, device='cuda:0')\n",
            "55.433333333333294\n",
            "tensor(28, device='cuda:0')\n",
            "56.366666666666625\n",
            "tensor(29, device='cuda:0')\n",
            "57.33333333333329\n",
            "tensor(29, device='cuda:0')\n",
            "58.29999999999996\n",
            "tensor(27, device='cuda:0')\n",
            "59.19999999999996\n",
            "tensor(26, device='cuda:0')\n",
            "60.06666666666663\n",
            "tensor(28, device='cuda:0')\n",
            "60.99999999999996\n",
            "tensor(26, device='cuda:0')\n",
            "61.866666666666625\n",
            "tensor(28, device='cuda:0')\n",
            "62.799999999999955\n",
            "tensor(28, device='cuda:0')\n",
            "63.733333333333285\n",
            "tensor(28, device='cuda:0')\n",
            "0.9333333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "1.9\n",
            "tensor(26, device='cuda:0')\n",
            "2.7666666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "3.7\n",
            "tensor(27, device='cuda:0')\n",
            "4.6000000000000005\n",
            "tensor(29, device='cuda:0')\n",
            "5.566666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "6.500000000000001\n",
            "tensor(30, device='cuda:0')\n",
            "7.500000000000001\n",
            "tensor(28, device='cuda:0')\n",
            "8.433333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "9.4\n",
            "tensor(28, device='cuda:0')\n",
            "10.333333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "11.266666666666667\n",
            "tensor(30, device='cuda:0')\n",
            "12.266666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "13.233333333333334\n",
            "tensor(30, device='cuda:0')\n",
            "14.233333333333334\n",
            "tensor(30, device='cuda:0')\n",
            "15.233333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "16.2\n",
            "tensor(28, device='cuda:0')\n",
            "17.133333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "18.099999999999998\n",
            "tensor(27, device='cuda:0')\n",
            "18.999999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "19.999999999999996\n",
            "tensor(27, device='cuda:0')\n",
            "20.899999999999995\n",
            "tensor(29, device='cuda:0')\n",
            "21.86666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "22.833333333333325\n",
            "tensor(28, device='cuda:0')\n",
            "23.76666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "24.699999999999992\n",
            "tensor(28, device='cuda:0')\n",
            "25.633333333333326\n",
            "tensor(28, device='cuda:0')\n",
            "26.56666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "27.499999999999993\n",
            "tensor(29, device='cuda:0')\n",
            "28.466666666666658\n",
            "tensor(30, device='cuda:0')\n",
            "29.466666666666658\n",
            "tensor(27, device='cuda:0')\n",
            "30.366666666666656\n",
            "tensor(28, device='cuda:0')\n",
            "31.29999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "32.19999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "33.16666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "34.133333333333326\n",
            "tensor(30, device='cuda:0')\n",
            "35.133333333333326\n",
            "tensor(26, device='cuda:0')\n",
            "35.99999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "36.93333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "37.89999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "38.86666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "39.86666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "40.76666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "41.76666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "42.73333333333333\n",
            "tensor(26, device='cuda:0')\n",
            "43.599999999999994\n",
            "tensor(29, device='cuda:0')\n",
            "44.56666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "45.53333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "46.46666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "47.46666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "48.36666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "49.29999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "50.26666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "51.26666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "52.16666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "53.066666666666656\n",
            "tensor(26, device='cuda:0')\n",
            "53.93333333333332\n",
            "tensor(25, device='cuda:0')\n",
            "54.76666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "55.73333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "56.699999999999996\n",
            "tensor(28, device='cuda:0')\n",
            "57.633333333333326\n",
            "tensor(29, device='cuda:0')\n",
            "58.599999999999994\n",
            "tensor(28, device='cuda:0')\n",
            "59.533333333333324\n",
            "tensor(28, device='cuda:0')\n",
            "60.466666666666654\n",
            "tensor(30, device='cuda:0')\n",
            "61.466666666666654\n",
            "tensor(29, device='cuda:0')\n",
            "62.43333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "63.39999999999999\n",
            "tensor(27, device='cuda:0')\n",
            "64.3\n",
            "tensor(29, device='cuda:0')\n",
            "0.9666666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "1.9333333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "2.8666666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "3.7666666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "4.733333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "5.733333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "6.633333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "7.566666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "8.533333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "9.5\n",
            "tensor(28, device='cuda:0')\n",
            "10.433333333333334\n",
            "tensor(30, device='cuda:0')\n",
            "11.433333333333334\n",
            "tensor(27, device='cuda:0')\n",
            "12.333333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "13.3\n",
            "tensor(29, device='cuda:0')\n",
            "14.266666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "15.233333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "16.2\n",
            "tensor(28, device='cuda:0')\n",
            "17.133333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "18.03333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "18.999999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "19.999999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "20.999999999999996\n",
            "tensor(29, device='cuda:0')\n",
            "21.96666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "22.96666666666666\n",
            "tensor(30, device='cuda:0')\n",
            "23.96666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "24.899999999999995\n",
            "tensor(27, device='cuda:0')\n",
            "25.799999999999994\n",
            "tensor(30, device='cuda:0')\n",
            "26.799999999999994\n",
            "tensor(28, device='cuda:0')\n",
            "27.733333333333327\n",
            "tensor(29, device='cuda:0')\n",
            "28.699999999999992\n",
            "tensor(29, device='cuda:0')\n",
            "29.666666666666657\n",
            "tensor(28, device='cuda:0')\n",
            "30.59999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "31.566666666666656\n",
            "tensor(30, device='cuda:0')\n",
            "32.566666666666656\n",
            "tensor(26, device='cuda:0')\n",
            "33.43333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "34.39999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "35.33333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "36.26666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "37.16666666666665\n",
            "tensor(30, device='cuda:0')\n",
            "38.16666666666665\n",
            "tensor(27, device='cuda:0')\n",
            "39.06666666666665\n",
            "tensor(26, device='cuda:0')\n",
            "39.933333333333316\n",
            "tensor(27, device='cuda:0')\n",
            "40.833333333333314\n",
            "tensor(29, device='cuda:0')\n",
            "41.79999999999998\n",
            "tensor(30, device='cuda:0')\n",
            "42.79999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "43.73333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "44.66666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "45.59999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "46.56666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "47.49999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "48.46666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "49.39999999999997\n",
            "tensor(30, device='cuda:0')\n",
            "50.39999999999997\n",
            "tensor(30, device='cuda:0')\n",
            "51.39999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "52.29999999999997\n",
            "tensor(30, device='cuda:0')\n",
            "53.29999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "54.2333333333333\n",
            "tensor(26, device='cuda:0')\n",
            "55.099999999999966\n",
            "tensor(29, device='cuda:0')\n",
            "56.066666666666634\n",
            "tensor(25, device='cuda:0')\n",
            "56.89999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "57.8333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "58.79999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "59.7333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "60.6333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "61.533333333333296\n",
            "tensor(27, device='cuda:0')\n",
            "62.433333333333294\n",
            "tensor(28, device='cuda:0')\n",
            "63.366666666666625\n",
            "tensor(27, device='cuda:0')\n",
            "64.26666666666662\n",
            "tensor(29, device='cuda:0')\n",
            "0.9666666666666667\n",
            "tensor(28, device='cuda:0')\n",
            "1.9\n",
            "tensor(29, device='cuda:0')\n",
            "2.8666666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "3.8333333333333335\n",
            "tensor(30, device='cuda:0')\n",
            "4.833333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "5.7666666666666675\n",
            "tensor(28, device='cuda:0')\n",
            "6.700000000000001\n",
            "tensor(30, device='cuda:0')\n",
            "7.700000000000001\n",
            "tensor(30, device='cuda:0')\n",
            "8.700000000000001\n",
            "tensor(27, device='cuda:0')\n",
            "9.600000000000001\n",
            "tensor(29, device='cuda:0')\n",
            "10.566666666666668\n",
            "tensor(30, device='cuda:0')\n",
            "11.566666666666668\n",
            "tensor(29, device='cuda:0')\n",
            "12.533333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "13.500000000000002\n",
            "tensor(26, device='cuda:0')\n",
            "14.366666666666669\n",
            "tensor(28, device='cuda:0')\n",
            "15.300000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "16.233333333333334\n",
            "tensor(28, device='cuda:0')\n",
            "17.166666666666668\n",
            "tensor(29, device='cuda:0')\n",
            "18.133333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "19.099999999999998\n",
            "tensor(29, device='cuda:0')\n",
            "20.066666666666663\n",
            "tensor(30, device='cuda:0')\n",
            "21.066666666666663\n",
            "tensor(28, device='cuda:0')\n",
            "21.999999999999996\n",
            "tensor(29, device='cuda:0')\n",
            "22.96666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "23.933333333333326\n",
            "tensor(28, device='cuda:0')\n",
            "24.86666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "25.799999999999994\n",
            "tensor(30, device='cuda:0')\n",
            "26.799999999999994\n",
            "tensor(29, device='cuda:0')\n",
            "27.76666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "28.733333333333324\n",
            "tensor(28, device='cuda:0')\n",
            "29.666666666666657\n",
            "tensor(28, device='cuda:0')\n",
            "30.59999999999999\n",
            "tensor(30, device='cuda:0')\n",
            "31.59999999999999\n",
            "tensor(29, device='cuda:0')\n",
            "32.566666666666656\n",
            "tensor(26, device='cuda:0')\n",
            "33.43333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "34.36666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "35.29999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "36.23333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "37.16666666666664\n",
            "tensor(30, device='cuda:0')\n",
            "38.16666666666664\n",
            "tensor(30, device='cuda:0')\n",
            "39.16666666666664\n",
            "tensor(27, device='cuda:0')\n",
            "40.06666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "40.99999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "41.89999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "42.8333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "43.79999999999997\n",
            "tensor(27, device='cuda:0')\n",
            "44.69999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "45.666666666666636\n",
            "tensor(27, device='cuda:0')\n",
            "46.566666666666634\n",
            "tensor(27, device='cuda:0')\n",
            "47.46666666666663\n",
            "tensor(28, device='cuda:0')\n",
            "48.39999999999996\n",
            "tensor(28, device='cuda:0')\n",
            "49.33333333333329\n",
            "tensor(30, device='cuda:0')\n",
            "50.33333333333329\n",
            "tensor(26, device='cuda:0')\n",
            "51.19999999999996\n",
            "tensor(28, device='cuda:0')\n",
            "52.13333333333329\n",
            "tensor(28, device='cuda:0')\n",
            "53.06666666666662\n",
            "tensor(28, device='cuda:0')\n",
            "53.99999999999995\n",
            "tensor(27, device='cuda:0')\n",
            "54.89999999999995\n",
            "tensor(27, device='cuda:0')\n",
            "55.79999999999995\n",
            "tensor(28, device='cuda:0')\n",
            "56.73333333333328\n",
            "tensor(28, device='cuda:0')\n",
            "57.66666666666661\n",
            "tensor(26, device='cuda:0')\n",
            "58.533333333333275\n",
            "tensor(28, device='cuda:0')\n",
            "59.466666666666605\n",
            "tensor(28, device='cuda:0')\n",
            "60.399999999999935\n",
            "tensor(28, device='cuda:0')\n",
            "61.333333333333265\n",
            "tensor(29, device='cuda:0')\n",
            "62.29999999999993\n",
            "tensor(30, device='cuda:0')\n",
            "63.29999999999993\n",
            "tensor(30, device='cuda:0')\n",
            "64.29999999999993\n",
            "tensor(28, device='cuda:0')\n",
            "0.9333333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "1.9333333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "2.9333333333333336\n",
            "tensor(30, device='cuda:0')\n",
            "3.9333333333333336\n",
            "tensor(29, device='cuda:0')\n",
            "4.9\n",
            "tensor(29, device='cuda:0')\n",
            "5.866666666666667\n",
            "tensor(29, device='cuda:0')\n",
            "6.833333333333334\n",
            "tensor(29, device='cuda:0')\n",
            "7.800000000000001\n",
            "tensor(27, device='cuda:0')\n",
            "8.700000000000001\n",
            "tensor(30, device='cuda:0')\n",
            "9.700000000000001\n",
            "tensor(30, device='cuda:0')\n",
            "10.700000000000001\n",
            "tensor(27, device='cuda:0')\n",
            "11.600000000000001\n",
            "tensor(29, device='cuda:0')\n",
            "12.566666666666668\n",
            "tensor(28, device='cuda:0')\n",
            "13.500000000000002\n",
            "tensor(28, device='cuda:0')\n",
            "14.433333333333335\n",
            "tensor(29, device='cuda:0')\n",
            "15.400000000000002\n",
            "tensor(29, device='cuda:0')\n",
            "16.366666666666667\n",
            "tensor(27, device='cuda:0')\n",
            "17.266666666666666\n",
            "tensor(29, device='cuda:0')\n",
            "18.23333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "19.23333333333333\n",
            "tensor(30, device='cuda:0')\n",
            "20.23333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "21.199999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "22.199999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "23.199999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "24.199999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "25.199999999999996\n",
            "tensor(30, device='cuda:0')\n",
            "26.199999999999996\n",
            "tensor(29, device='cuda:0')\n",
            "27.16666666666666\n",
            "tensor(27, device='cuda:0')\n",
            "28.06666666666666\n",
            "tensor(28, device='cuda:0')\n",
            "28.999999999999993\n",
            "tensor(30, device='cuda:0')\n",
            "29.999999999999993\n",
            "tensor(28, device='cuda:0')\n",
            "30.933333333333326\n",
            "tensor(29, device='cuda:0')\n",
            "31.89999999999999\n",
            "tensor(28, device='cuda:0')\n",
            "32.83333333333332\n",
            "tensor(27, device='cuda:0')\n",
            "33.73333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "34.66666666666665\n",
            "tensor(30, device='cuda:0')\n",
            "35.66666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "36.63333333333332\n",
            "tensor(30, device='cuda:0')\n",
            "37.63333333333332\n",
            "tensor(28, device='cuda:0')\n",
            "38.56666666666665\n",
            "tensor(29, device='cuda:0')\n",
            "39.53333333333332\n",
            "tensor(29, device='cuda:0')\n",
            "40.499999999999986\n",
            "tensor(29, device='cuda:0')\n",
            "41.466666666666654\n",
            "tensor(28, device='cuda:0')\n",
            "42.399999999999984\n",
            "tensor(30, device='cuda:0')\n",
            "43.399999999999984\n",
            "tensor(29, device='cuda:0')\n",
            "44.36666666666665\n",
            "tensor(28, device='cuda:0')\n",
            "45.29999999999998\n",
            "tensor(30, device='cuda:0')\n",
            "46.29999999999998\n",
            "tensor(28, device='cuda:0')\n",
            "47.23333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "48.16666666666664\n",
            "tensor(27, device='cuda:0')\n",
            "49.06666666666664\n",
            "tensor(29, device='cuda:0')\n",
            "50.03333333333331\n",
            "tensor(28, device='cuda:0')\n",
            "50.96666666666664\n",
            "tensor(28, device='cuda:0')\n",
            "51.89999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "52.86666666666664\n",
            "tensor(26, device='cuda:0')\n",
            "53.733333333333306\n",
            "tensor(27, device='cuda:0')\n",
            "54.633333333333304\n",
            "tensor(30, device='cuda:0')\n",
            "55.633333333333304\n",
            "tensor(30, device='cuda:0')\n",
            "56.633333333333304\n",
            "tensor(29, device='cuda:0')\n",
            "57.59999999999997\n",
            "tensor(28, device='cuda:0')\n",
            "58.5333333333333\n",
            "tensor(28, device='cuda:0')\n",
            "59.46666666666663\n",
            "tensor(30, device='cuda:0')\n",
            "60.46666666666663\n",
            "tensor(29, device='cuda:0')\n",
            "61.4333333333333\n",
            "tensor(27, device='cuda:0')\n",
            "62.3333333333333\n",
            "tensor(29, device='cuda:0')\n",
            "63.29999999999997\n",
            "tensor(29, device='cuda:0')\n",
            "64.26666666666664\n",
            "tensor(30, device='cuda:0')\n",
            "65.26666666666664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt7viypNVxIo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}