{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DH-N5iTbEdGf",
    "outputId": "2a5b84f4-e636-4397-d35b-b2c22ce43205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive. Make sure you have datasets at root directory in gdrive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0irrEnTeHBZ",
    "outputId": "a35732fa-6780-4fdd-8328-736e83c3d730"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryanhubley/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6Tt6Ya1SHet"
   },
   "outputs": [],
   "source": [
    "def preprocess_func(sentences):\n",
    "  \"\"\" \n",
    "  args: sentences => List of sentences that need to be preprocessed\n",
    "  returns: cleaned_sentences => List of cleaned sentences that are lower-case, punctuation removed, and stopwords removed\"\"\"\n",
    "  meaninglessWords=list(stopwords.words('english'))\n",
    "  cleaned_sentences = []\n",
    "  sents = sentences\n",
    "  for i in range(len(sents)):\n",
    "    sent = sents[i]\n",
    "    sent = sent.lower() #convert to lowercase\n",
    "    for char in sent:\n",
    "      if char in string.punctuation:\n",
    "        sent=sent.replace(char,\"\") #remove punctutation\n",
    "    words = sent.split()\n",
    "    sent = []\n",
    "    for word in words:\n",
    "      if word not in meaninglessWords:\n",
    "        sent.append(word)\n",
    "    sent = \" \".join(sent) #remove stopwords\n",
    "    cleaned_sentences.append(sent)\n",
    "  return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-G8MZZRpFrRU"
   },
   "outputs": [],
   "source": [
    "# Loading in review datasets. The txt files hav the sentence and the label, separated by a tab (\\t)\n",
    "\n",
    "# For google colab\n",
    "#amazon_df = pd.read_csv('/content/gdrive/My Drive/amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "#yelp_df = pd.read_csv('/content/gdrive/My Drive/yelp_labelled.txt', sep='\\t', header=None)\n",
    "#imdb_df = pd.read_csv('/content/gdrive/My Drive/imdb_labelled.txt', sep='\\t', header=None)\n",
    "\n",
    "# For local\n",
    "amazon_df = pd.read_csv('./amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "yelp_df = pd.read_csv('./yelp_labelled.txt', sep='\\t', header=None)\n",
    "imdb_df = pd.read_csv('./imdb_labelled.txt', sep='\\t', header=None)\n",
    "\n",
    "\n",
    "# Cleaning and concatenating reviews\n",
    "cleaned_sentencesAmazon = preprocess_func(list(amazon_df[0]))\n",
    "cleaned_sentencesYelp = preprocess_func(list(yelp_df[0]))\n",
    "cleaned_sentencesIMDB = preprocess_func(list(imdb_df[0]))\n",
    "amazon_df['Cleaned Sentences'] = cleaned_sentencesAmazon\n",
    "yelp_df['Cleaned Sentences'] = cleaned_sentencesYelp\n",
    "imdb_df['Cleaned Sentences'] = cleaned_sentencesIMDB\n",
    "amazon_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
    "amazon_df = amazon_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
    "yelp_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
    "yelp_df = yelp_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
    "imdb_df.columns = ['Sentence', 'Label', 'Cleaned Sentences']\n",
    "imdb_df = imdb_df[['Sentence', 'Cleaned Sentences', 'Label']]\n",
    "amazon_df = amazon_df.drop(labels='Sentence', axis=1)\n",
    "amazon_df.columns=['Sentence', 'Label']\n",
    "yelp_df = yelp_df.drop(labels='Sentence', axis=1)\n",
    "yelp_df.columns=['Sentence', 'Label']\n",
    "imdb_df = imdb_df.drop(labels='Sentence', axis=1)\n",
    "imdb_df.columns = ['Sentence', 'Label']\n",
    "\n",
    "df_reviews = pd.concat([amazon_df, yelp_df, imdb_df])\n",
    "df_reviews.columns = ['Sentence', 'Label']\n",
    "\n",
    "# Loading in sentiment140 dataset. In this dataset there are 3 labels (0 = negative, 2 = neutral, 4 = positive)\n",
    "#df_tweets = pd.read_csv('/content/gdrive/My Drive/sentiment140.csv', encoding='latin-1', header=None)\n",
    "df_tweets = pd.read_csv('./sentiment140.csv', encoding='latin-1', header=None)\n",
    "df_tweets = df_tweets.drop(columns=[1,2,3,4])\n",
    "df_tweets.columns = ['Label', 'Sentence']\n",
    "df_tweets.drop(df_tweets[df_tweets['Label']==2].index, inplace=True)\n",
    "df_tweets['Label'] = df_tweets['Label'].replace(4,1)\n",
    "\n",
    "# Tweet cleaning\n",
    "meaninglessWords=list(stopwords.words('english'))\n",
    "\n",
    "# Remove @'s\n",
    "df_tweets[\"Sentence\"] = df_tweets[\"Sentence\"].str.replace(\"(\\@\\w+.*?)\",\"\")\n",
    "# Remove links\n",
    "df_tweets[\"Sentence\"] = df_tweets[\"Sentence\"].str.replace(r\"http\\S+\",\"\")\n",
    "# To lowercase\n",
    "df_tweets[\"Sentence\"] = df_tweets[\"Sentence\"].str.lower()\n",
    "# Remove stopwords\n",
    "df_tweets[\"Sentence\"] = df_tweets[\"Sentence\"].apply(lambda x: \" \".join([item for item in x.split() if item not in meaninglessWords]))\n",
    "# Remove punctuation\n",
    "df_tweets[\"Sentence\"] = df_tweets[\"Sentence\"].apply(lambda x: \"\".join([item for item in x if item not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "5LVYk6DhIM8o",
    "outputId": "dfac2bf2-acf3-4f22-acad-320d41a18692"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>way plug us unless go converter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>great jawbone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Sentence  Label\n",
       "0  way plug us unless go converter      0\n",
       "1        good case excellent value      1\n",
       "2                    great jawbone      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data examples in reviews dataset\n",
    "df_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "hpsaaNBzJPnM",
    "outputId": "9fdc702b-ce4b-4bf5-bc84-ec73379c7040"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr thir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting it might cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>dived many times ball managed save 50 rest go ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                           Sentence\n",
       "0      0   awww thats bummer shoulda got david carr thir...\n",
       "1      0  upset cant update facebook texting it might cr...\n",
       "2      0  dived many times ball managed save 50 rest go ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data examples in tweet dataset\n",
    "df_tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I1oTVA_jWG7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJrq0uTMe1AB",
    "outputId": "11889937-3874-4f8b-e69f-2a3fd359fdba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60825625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram NB trained with reviews tested on tweets\n",
    "unigram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "unigram_NB.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = unigram_NB.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4e23HuMnmz_",
    "outputId": "b2c6aa57-d980-4eae-aebc-c95c3c7d4165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651018922852984"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram NB trained with tweets tested on reviews\n",
    "unigram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "# Reduced train size to comparable amount of reviews that we have\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "unigram_NB.fit(x_train, y_train)\n",
    "predicted = unigram_NB.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUbGcYaPjRf5",
    "outputId": "93fe5e68-0935-4ea1-96a0-597fcd7a1d87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.610893125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram and Bigram NB trained with reviews tested on tweets\n",
    "uniAndBigram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "uniAndBigram_NB.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = uniAndBigram_NB.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LLIMCnHjwX5",
    "outputId": "fd797517-06ec-4a18-ae49-1625329f8c73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6786754002911208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram and Bigram NB trained with tweets and tested on reviews\n",
    "uniAndBigram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "uniAndBigram_NB.fit(x_train, y_train)\n",
    "predicted = uniAndBigram_NB.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjIFUYDBljjD",
    "outputId": "00451ca6-a37e-45ca-953b-667a81fe1875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.610946875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram, Bigram, and Trigram NB trained with reviews tested on tweets\n",
    "ubtGram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "ubtGram_NB.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = ubtGram_NB.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvxiuA4AmJ4Y",
    "outputId": "7bdf63f9-5a64-4a66-fafa-ce757b08b956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6750363901018923"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram, Bigram, and Trigram NB trained with tweets and tested on reviews\n",
    "ubtGram_NB = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "ubtGram_NB.fit(x_train, y_train)\n",
    "predicted = ubtGram_NB.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_SDc7nxqHQa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN9Lt_M5p6eZ",
    "outputId": "26561325-f5d5-4249-9f33-464215d92741"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.586419375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram SVM trained with reviews tested on tweets\n",
    "unigram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "unigram_SVM.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = unigram_SVM.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPpGGrw6p6ea",
    "outputId": "261ab898-7253-40ac-c156-7754309114ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6095342066957787"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram SVM trained with tweets tested on reviews\n",
    "unigram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "# Reduced train size to comparable amount of reviews that we have\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "unigram_SVM.fit(x_train, y_train)\n",
    "predicted = unigram_SVM.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOaSu8E7p6ea",
    "outputId": "b751d3f8-1b4b-46b5-e71d-8fc27ba63ef3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59957"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram and Bigram SVM trained with reviews tested on tweets\n",
    "uniAndBigram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "uniAndBigram_SVM.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = uniAndBigram_SVM.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-axhinb3p6eb",
    "outputId": "454ddb51-9a97-434b-a3d4-7bff598144ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6706695778748181"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram and Bigram NB trained with tweets and tested on reviews\n",
    "uniAndBigram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "uniAndBigram_SVM.fit(x_train, y_train)\n",
    "predicted = uniAndBigram_SVM.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABLjhF7vp6eb",
    "outputId": "cfc043bf-1f70-4b96-f062-5665a7417657"
   },
   "outputs": [],
   "source": [
    "# Unigram, Bigram, and Trigram NB trained with reviews tested on tweets\n",
    "ubtGram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "ubtGram_SVM.fit(df_reviews['Sentence'], df_reviews['Label'])\n",
    "predicted = ubtGram_SVM.predict(df_tweets['Sentence'])\n",
    "np.mean(predicted == df_tweets['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaoGHryUp6eb",
    "outputId": "81804f37-dcec-451b-fb92-d2ea5025ab26"
   },
   "outputs": [],
   "source": [
    "# Unigram, Bigram, and Trigram NB trained with tweets and tested on reviews\n",
    "ubtGram_SVM = Pipeline([\n",
    "     ('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier()),\n",
    " ])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tweets['Sentence'], df_tweets['Label'], test_size=0.9981)\n",
    "\n",
    "ubtGram_SVM.fit(x_train, y_train)\n",
    "predicted = ubtGram_SVM.predict(df_reviews['Sentence'])\n",
    "np.mean(predicted == df_reviews['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "crossover_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
